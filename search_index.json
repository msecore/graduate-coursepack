[["index.html", "Northwestern University Materials Science and Engineering Graduate Core Coursepack Abstract Disclaimer Intended Outcomes Multimedia Content Northwestern Materials Science and Engineering eReserves", " Northwestern University Materials Science and Engineering Graduate Core Coursepack Jonathan D. Emery and Lincoln J. Lauhon 2021-07-16 Abstract This coursepack is intended to serve matriculating graduate students as a resource for the foun-dational knowledge necessary to succeed in the Materials Science and Engineering Department’sgraduate core curriculum. Students should review this coursepack prior to and during their en-rollment. This coursepack contains references and original material with focus on MaterialsScience and Engineering background, mathematics, and coding. Disclaimer The coursepack is intended to be an evergreen document. Improvements, corrections, and updates will be completed with feedback from graduate students and core instructors. Tentative release dates for new material are noted in section headings. Students and instructors are invited to comment directly on the PDF or to contact Jonathan Emery at jonathan.emery@northwestern.edu with suggestions. Intended Outcomes Students that study this coursepack and referenced documents should be able to: Demonstrate familiarity with foundational undergraduate-level Materials Science concepts as covered in Chapter 1 of Kasap(Kasap 2006). Navigate constitutive relations using tensor notation and operations. Employ various techniques — such as separation of variables and Fourier analysis — to solve introductory boundary value problems (e.g. diffusion, heat transfer, linear elasticity). Construct rudimentary MATLAB or Mathematica scripts to solve problems and visualize/analyze data. Multimedia Content This document contains multimedia content including video and imbedded interactive 3D graphics. This content is supported through Adobe products (e.g. Adobe Acrobat Reader DC), but the reader must approve the use of multimedia content in Adobe. This can be done when opening the document for the first time: Allow Multimedia Content or by navigating to Edit \\(\\rightarrow\\) Preferences \\(\\rightarrow\\) Multimedia Trust: Allow Multimedia Operations. Interaction with multimedia is done directly within the PDF. Click on content to play short videos or animations. Click-and-twist to rotate 3D interactive content such as crystal structures. Northwestern Materials Science and Engineering eReserves The department has made available portions of select introductory texts suitable for students who have limited background in materials science. These texts are stored in an online repository hosted on Northwestern box, and are available to all Northwestern Materials Science students here. Brief descriptions of these texts are available in Sec. \\[sec:Intro\\]. By utilizing these texts, you are agreeing to Northwestern’s policies on eReserve, which are compliant with US copyright law (Title 17, United States Code): \\(\\left[\\text{Sources are}\\right]\\) not to be used for any purpose other than private study, scholarship, or research. If electronic transmission of reserve material is used for purposes in excess of what constitutes “fair use,” that user may be liable for copyright infringement. References "],["the-graduate-core.html", "1 The Graduate Core 1.1 MAT_SCI 401: Chemical &amp; Statistical Thermodynamics 1.2 MAT_SCI 402: Structure of Crystalline and Non-crystalline Materials 1.3 MAT_SCI 404: Imperfections in Materials 1.4 MAT_SCI 405: Physics of Solids 1.5 MAT_SCI 406: Symmetry and Mechanical Properties of Materials 1.6 MAT_SCI 408: Phase Transformations in Materials", " 1 The Graduate Core 1.1 MAT_SCI 401: Chemical &amp; Statistical Thermodynamics Course Description: The purpose of this course is to introduce the principles of statistical thermodynamics and to demonstrate how it provides quantitative understanding of a broad range of phenomena in materials science, including phase equilibria, the behavior of solutions and mixtures, kinetics, etc. The course provides a important foundation for subsequent courses in Materials Science and Engineering as well as future research in a wide range of areas. The following topics in classical thermodynamics will be covered: The laws of thermodynamics; conditions for equilibrium (ref: Topic401-1) Solutions (ref: Topic401-2) Excess quantities (ref: Topic401-3) Binary and ternary phase diagrams (ref: Topic401-4) Statistical definition of entropy (ref: Topic401-5) Ensembles and the Boltzmann and Gibbs distributions (ref: Topic401-6) Quantum and classical ideal gasses (ref: Topic401-7) The regular solution model (ref: Topic401-8) Textbook (Dill and Bromberg 2012) 1.2 MAT_SCI 402: Structure of Crystalline and Non-crystalline Materials Course Description This course centers on the methods of describing both crystalline and non-crystalline materials, as well as addresses the underlying relationships of materials and their structures. Symmetry principles and tensor representations of crystal properties will be used to describe, for example, transport behavior, elasticity, displacement phase transitions, and material anisotropy. Topics in non-crystalline solids include theories and models of amorphousness, as well as influence of ordering in hard and soft condensed matter systems such as glasses and polymers. Symmetry Principles, Point, Plane, and Space Groups (ref: Topic402-1) Crystallographic Descriptors and Structure (ref: Topic402-2) Material Anisotropy (ref: Topic402-3) Displacive Phase Transitions (ref: Topic402-4) Tensors and Constitutive Relations (ref: Topic402-5) Sphere Packing and Non-crystalline Solids (ref: Topic402-6) Polymer Chemistry and Architecture (ref: Topic402-7) Molecular Order in Soft Condensed Matter (ref: Topic402-8) Main Texts: (Nye and others 1985) (De Graef and McHenry 2012) (Jones et al. 2002) 1.3 MAT_SCI 404: Imperfections in Materials Course Description: A graduate course in defects and imperfections in crystalline material, with special emphasis on dislocations and mechanical behavior of metals. Point Defect Mechanics Point Defect Thermodynamics Point Defect Equilibrium Point Defect Kinetics Dislocation Geometry Dislocation Mechanics Dislocation Interactions and Applications Partial and Extended Dislocations Dislocation Core Structure Grain Boundary Geometry Grain Boundary Mechanics Main Texts: (Cai and Nix 2016) (Hull and Bacon 2011) Supplementary Texts: (Shewmon 2016) (Glicksman 2000) (Weertman and Weertman 1964) (Hirth and Lothe 1982) 1.4 MAT_SCI 405: Physics of Solids Course Description: This course provides an overview of solid state physics. Topics include: Free electron theory Phonons Energy bands Charge transport Semiconductors Optical properties Dielectric properties Ferroelectrics Diagmagnetism Paramagnetism Magnetic ordering Textbooks: (???) (Kittel 2005) Prerequisites: Students should have a MAT_SCI 351-1 and MAT_SCI 351-2 or equivalent background in introductory quantum mechanics and solid-state physics. Approach the instructor with questions if you are unsure about your background. 1.5 MAT_SCI 406: Symmetry and Mechanical Properties of Materials Course Description: The relation between symmetry of structure and symmetry of properties is emphasized. The following topics will be covered: Stress, strain, and linear elasticity Effect of defects on deformation mechanisms, modeling of dislocation motion Plastic deformation at low temperature and strengthening mechanisms Plastic deformation at high temperature, and creep resistance mechanisms Fracture under monotonic loading and toughening mechanisms Fracture under cyclical stresses and fatigue resistance approach Environmental effects on fracture modes Introduction to Finite Elements Textbooks: (Courtney 2005) 1.6 MAT_SCI 408: Phase Transformations in Materials 1.6.1 Course Description: {#course-description .unnumbered] This course will cover a variety of topics associated with the thermodynamics and kinetics of phase transformations in materials. Topics to be covered include: Thermodynamics of interfaces, capillarity, and homogenous nucleation Dynamics of nucleation, heterogenous nucleation Composition gradients and free energy Interfacial energy of diffuse interfaces Diffusion and the diffusion equations The Cahn-Hilliard equation and spinodal decomposition Johnson-Mehl-Avrami-Kolmogorov theory Similarity solution for particle growth by diffusion Coarsening/Ostwald ripening Order parameters: conserved and unconserved Main Texts: (Hoyt 2011) Supplementary Texts: (Balluffi, Allen, and Carter 2005) (Porter and Easterling 2009) References "],["introduction-to-materials-science.html", "2 Introduction to Materials Science", " 2 Introduction to Materials Science For students who completed their undergraduate studies in fields other than Materials Science and Engineering (MSE), we recommend a thorough review of one or more introductory texts. This review will provide exposure to concepts that may not have been previously encountered, and allowing for more advanced study within the MSE core. The references below are recommended introductory, undergraduate-level (200- or 300-level) texts. A limited number of these texts are available through the Northwestern Engineering Library, but students may find that their advisors and lab-mates may have extra copies on-hand. We have deposited some of these texts onto the department’s electronic reserve, hosted here, or find it on the Materials Science Canvas site under \\Files\\eReserve. Principles of Electronic Materials and Devices, Ed. 3, Ch. 1 Safa O. Kasap, McGraw Hill, 2006: Resource: Ch. 1 of Kasap is available on the eReserve. Principles of Electronic Materials and Devices is a text used in Northwestern’s undergraduate Physics of Materials courses (MAT_SCI 351-1). While focusing on electronic structure and properties, Kasap spends the first 100 pages providing a broad, conceptual review of important topics in Materials Science and Engineering. This is the minimum suggested reading for those with no background in MSE and is provided in the eReserve. Suggested Reading: Kasap touches briefly on many introductory MSE core topics. A read through this material will provide familiarity with concepts that will arise in the MAT_SCI graduate core. Courses in which you will encounter these topics are listed in parentheses. Pgs. 9-25: Bonding and Materials Classification (401, 404, 405, 406, 408). Pgs. 25-36: Kinetic Molecular Theory (401, 408). Pgs. 40-45: Heat and Thermal Fluctuations (401, 404, 405, 408). Pgs. 45-49: Thermally Activated Processes (401, 404, 405, 406, 408). Pgs. 49-63: Crystal Structures (404, 405, 406, 408) Pgs. 64-76: Crystalline Defects (404, 406, 408). Pgs. 78-82: Glasses and Amorphous Materials (401, 408). Pgs. 78-82: Glasses and Amorphous Materials (401, 408). Pgs. 83-94: Solid Solutions and Two-Phase Solids (401, 408) Phase Transformations in Metals and Alloys, Ed. 3, Ch. 1 Porter, Easterling, and Sherif, Chapman and Hall, 2008 Resource: Ch. 1 of Porter and Easterling is available in the eReserves. Porting and Easterling is an text introducing phase transformations for 3rd and 4th year undergraduates. Ch. 1 provides background information on thermodynamical principles central to MAT_SCI 401 and 408. Some of the material in Ch. 1 is covered in MAT_SCI 401, but students should be familiar with these concepts prior to enrollment in MAT_SCI 408. Materials Science and Engineering, an Introduction, Ed. 9e, William D. Callister, Jr. and David G. Rethwisch, Wiley, 2014: This text is often used in the Northwestern Materials Science undergraduate introduction course, taken in the 2nd year. Callister provides a basic introduction to core materials science concepts associated with structure, properties, processing, and performance. It is very detailed and not suggested as the best text for someone completely unfamiliar with MSE concepts. The text has a more specific focus on the details of metals processing than many introductory texts. Suggested Reading: A list of the core topics covered in an introductory MSE core course are listed below. The MAT_SCI graduate core courses in which you will encounter these topics are listed in parentheses. Ch. 2: Electronic structure and inter-atomic pair potentials (405). Ch. 3: Basic metallic crystalline structures and crystal navigation (404, 405, 406, 408). Ch. 4: Imperfections. Simple 0D, 1D, 2D and 3D defects in materials (404, 406, 408). Ch. 5: Diffusion. 1D formulations of Fick’s 1st and 2nd Laws (401, 404, 408). Ch. 6, 7, 8: Microstructural mechanical response: Elasticity, plasticity, and failure (404, 406). Ch. 9: Phase Diagrams: Simple phase diagram interpretation and the Gibbs Phase Rule (401, 408). Ch. 10: Kinetics in phase transformations and the equilibrium state (401, 408). Resource: There is no copy of this book in the MAT_SCI eReserves. However, there are dozens of copies floating around the department. Ask your lab-mates or your advisors for a copy. Introduction to Materials Science for Engineers, Ed. 8, James F. Shackelford, Pearson, 2015: Shackelford’s book is similar to that of Callister’s, but the content is more selective. This book is also more concise and takes a more conceptual approach to MSE than Callister. The problems are often composed to provide engineering context. Shackelford spends much less time on processing than Callister, but covers structure, properties and performance at the same level of detail. Resource: There is no copy of this book in the MAT_SCI eReserves. However, there are many copies floating around the department. Ask your lab-mates or advisors for a copy. "],["mathematics.html", "3 Mathematics 3.1 Linear Algebra 3.2 Tensors (Release 1/2017) 3.3 Summation Notation 3.4 Coordinate Transformations (Release 1/2017) 3.5 Calculus 3.6 Differential Equations", " 3 Mathematics The text below is not a rigorous approach to the mathematical theory, nor is it a wholly systematic or comprehensive description of the topics covered. It is a selection of topics recommended by core instructors. These include mathematical concepts and procedures that will be encountered in your core courses, and instructors expect you to be familiar with them prior to the beginning of a course, i.e, they will not cover them in detail. Instead, use the content below as a reference as these topics arise and as a platform for more in-depth study. Please contact jonathan.emery@northwestern.edu with suggestions for additional material to be included in this section. For those who have never seen the mathematics below or who are not comfortable with the material, further preparation may be necessary. Options for those students include: either a.) enroll in ES-APPM-311-1 and ES-APPM-311-2: Methods of Applied Mathematics and/or b.) utilizing the suggested resources for supplemental study. 3.1 Linear Algebra Linear algebra is a branch of mathematics that is central to physical description in Materials Science as it concerns the description of vectors spaces and is used in solving systems of equations. Materials Science graduate students will encounter application of linear algebra in all core courses. The sections below outline basic linear algebra concepts. 3.1.1 Linear Systems 3.1.2 Gauss Elimination (Release TBD) 3.1.2.1 MATrix Algebra and Operations (Release TBD) 3.1.3 Linear Transformations (Release TBD) 3.1.4 Determinants (Release TBD) 3.1.5 Eigenvalues and Eigenvectors (Release TBD) 3.1.6 Linear Differential Equations (Release TBD) Linear Differential Operators Linear Differential Equations 3.2 Tensors (Release 1/2017) Tensors are mathematical objects that define relationships between scalars, vectors, matrices, and other tensors=. Tensors are represented as arrays of various dimensionality (defined by rank or order). The moniker “tensor” in general suggests a higher-rank array (most often \\(\\geq\\)3 dimensions), but scalars, vectors, and matrices are also tensors. In the MSE graduate core, students will encounter tensors of various rank. In physical science, tensors characterize the properties of a physical system. Tensors are the de facto tool used to describe, for example, diffusion, nucleation and growth, states of stress and strain, Hamiltonians in quantum mechanics, and many, many, more physical phenomenon. Physical processes of interest to Materials Scientists take place in Euclidean 3-space (\\({\\rm I\\!R}^3\\)) are are well-described by tensor representations. We build up our description of the handling of tensors starting by separately describing rank-0, rank-1, rank-2, and rank-3 tensors. Tensors of lower ranks should be familiar — students will have encountered them previously as scalars (rank-0), vectors (rank-1), and matrices (rank-2). The term tensors typically denotes arrays of higher dimensionality (rank \\(\\geq3\\)). Physical examples include the rank-2 Cauchy stress tensor which describes the stress state of a at a point within a material), the rank-3 piezoelectric tensor (which relates the dielectric polarization of a material to a stress state), and the rank-4 stiffness tensor (which relates strain state and stress state in a system that obeying Hooke’s law). Classifications of tensors by rank allows us to quickly identify the number of tensor components we will work with: a tensor of order \\(p\\) has \\(N^p\\) components, where \\(N\\) is the dimensionality of space in which we are operating. In general, you will be operating in Eucledian 3-space, so the number of components of a tensor is defined as \\(3^p\\). Scalars are considered tensors with order or rank of 0. Scalars represent physical quantities (often accompanied by a unit of measurement) that possess only a magnitude: e.g., temperature, mass, charge, and distance. Scalars are typically represented by Latin or Greek symbols and have \\(3^{0} = 1\\) component. Vectors are tensors with a rank of 1. In symbolic notation, vectors are typically represented using lowercase bold or bold-italic symbols such as \\(\\mathbf{u}\\) or \\(\\pmb{a}\\). Bold typeface is not always amenable to handwriting, however, and so the a right arrow accent is employed: \\(\\vec{u}\\) or \\(\\vec{a}\\). Students are likely to encounter various conventions depending on their field of study. In \\({\\rm I\\!R}^3\\) a vector is defined by \\(3^{1} = 3\\) components. In xyz Cartesian coordinates we utilize the Cartesian basis with 3 orthogonal unit vectors \\(\\{\\mathbf{e}_{\\mathbf{x}}\\text{, } \\mathbf{e}_{\\mathbf{y}}\\text{, } \\mathbf{e}_{\\mathbf{z}}\\}\\). We define 3D vector \\(\\mathbf{u}\\) in this basis with the components (\\(u_x\\), \\(u_y\\), \\(u_z\\)), or equivalently (\\(u_1\\), \\(u_2\\), \\(u_3\\)). Often, we represent the vector \\(\\mathbf{u}\\) using the shorthand \\(u_i\\), where the \\(i\\) subscript denotes an index that ranges over the dimensionality of the system (1,2,3 for \\({\\rm I\\!R}^3\\), 1,2 for \\({\\rm I\\!R}^2\\)). Vectors are often encountered in a bracketed vertical list to facilitate matrix operations. Using some of the notation defined above: \\[\\mathbf{u} = u_i = \\begin{bmatrix} u_x \\\\ u_y \\\\ u_z \\end{bmatrix} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{bmatrix} \\label{eq:Vector}\\] Matrices are tensors with a rank of 2. In \\({\\rm I\\!R}^2\\) a matrix has \\(2^{2} = 4\\) components and in \\({\\rm I\\!R}^3\\) a matrix has \\(3^{2} = 9\\) components. As with vectors, we will use the range convention when denoting a matrix, which now possesses two subscripts, \\(i\\) and \\(j\\). We use the example of the true stress, or Cauchy stress tensor, \\(\\sigma_{ij}\\): \\[\\sigma_{ij} = \\begin{bmatrix} \\sigma_{xx} &amp; \\sigma_{xy} &amp; \\sigma_{xz}\\\\ \\sigma_{yx} &amp; \\sigma_{yy} &amp; \\sigma_{yz}\\\\ \\sigma_{zx} &amp; \\sigma_{zy} &amp; \\sigma_{zz}\\\\ \\end{bmatrix}\\] Where the diagonal represents the normal components of stress and the off-diagonal represents the shear components of the stress. In this notation the first index denotes the row while the second denotes the column (\\(x = 1\\), \\(y = 2\\), \\(z = 3\\)). Tensors A rank-3 tensor in \\({\\rm I\\!R}^3\\) has \\(3^{3} = 27\\) components and is represented in range notation using subscripts \\(i\\), \\(j\\), and \\(k\\), e.g., \\(T_{ijk}\\) . At rank-3 (and it is even worse in rank-4, requiring an array of rank-3 tensors) it begins to become difficult to represent clearly on paper. An example of a simple tensor — the rank-3 permutation tensor — is shown in Fig. 1. You can also watch this video which helps with the visualization. The rank-3 permutation tensor, by Arian Kriesch. corrections made by Xmaster1123 and Luxo (Own work) [GFDL (http://www.gnu.org/copyleft/fdl.html), CC-BY-SA-3.0 (http://creativecommons.org/licenses/by-sa/3.0/) One can write the \\(i = 1,2,3\\) matrices that stack to form this tensor as: \\[\\epsilon_{1jk}= \\begin{bmatrix} 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}\\] \\[\\epsilon_{2jk}= \\begin{bmatrix} 0 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}\\] \\[\\epsilon_{3jk}= \\begin{bmatrix} 0 &amp; 1 &amp; 0\\\\ -1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] 3.3 Summation Notation Often, it is useful to simplify notation when manipulating tensor equations. To do this, we utilize Einstein summation notation, or simply summation notation. This notation says that if an index is repeated twice (and only twice) in a single term we assume summation over the range of the repeated subscript. The simplest example of this is the representation of the trace of a matrix: \\[tr(\\sigma) = \\underbrace{\\sigma_{kk}}_{\\substack{\\text{summation} \\\\ \\text{notation}}} = \\sum_{k}^{3}\\sigma_{kk} = \\sigma_{11}+\\sigma_{22}+\\sigma_{33}\\] In \\(\\sigma_{kk}\\) the index \\(k\\) is repeated, and this means that we assume summation of the index over the range of the subscript (in this case, 1-3 as we are working with the stress tensor). Example 1:This comes in very useful when representing matrix multiplication. Let’s say we have an (\\(M \\times N\\)) matrix, \\(\\mathbf{A} = a_{ij}\\) and an \\(R \\times P\\) matrix \\(\\mathbf{B} = b_{ij}\\). We know from linear algebra that the matrix product \\(\\mathbf{AB}\\) is defined only when \\(R = N\\), and the result is a (\\(M \\times P\\)) matrix, \\(\\mathbf{C} = c_{ij}\\). Here’s an example with a (\\(2 \\times 3\\)) matrix times a (\\(3 \\times 2\\)) in conventional representation: \\[\\begin{aligned} \\mathbf{AB} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; a_{23}\\\\ \\end{bmatrix} &amp;\\begin{bmatrix} b_{11} &amp; b_{12}\\\\ b_{21} &amp; b_{22}\\\\ b_{31} &amp; b_{32}\\\\ \\end{bmatrix} = \\\\ &amp;\\begin{bmatrix} a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31} &amp; a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32}\\\\ a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31} &amp; a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}\\\\ \\end{bmatrix} =c_{ij}\\end{aligned}\\] Here, we can use summation notation to greatly simply the expression. The components of the matrix \\(c_{ij}\\) are \\(c_{11}\\), \\(c_{12}\\), \\(c_{21}\\), and \\(c_{22}\\) and are defined: \\[\\begin{aligned} c_{11} = a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}\\\\ c_{12} = a_{11}b_{12} + a_{12}b_{22} + a_{13}b_{32}\\\\ c_{21} = a_{21}b_{11} + a_{22}b_{21} + a_{23}b_{31}\\\\ c_{22} = a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}\\\\\\end{aligned}\\] These terms can all be represented using the following expression: \\[c_{ij} = \\sum_{k=1}^{3} a_{ik}b_{kj} = a_{i1}b_{1j} + a_{i2}b_{2j} + a_{i3}b_{3j}\\] So, in general for any matrix product: \\[c_{ij} = \\sum_{k=1}^{N} a_{ik}b_{kj} = a_{i1}b_{1j} + a_{i2}b_{2j} + \\cdots + a_{iN}b_{Nj} \\label{eq:MatrixMultiply}\\] Or, by dropping the summation symbol and fully utilizing the summation convention: \\[c_{ij} = a_{ik}b_{ki}\\] Note that the term \\(c_{ij}\\) has no repeated subscript - there is no summation implied here. It is simply a matrix. Summation is implied in the \\(a_{ik}b_{kj}\\) term because of the repeated index \\(k\\), often called the dummy index. Example 2: Another example is a \\(3 \\times 3\\) matrix multiplied by a (3 ) column vector: \\[\\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; a_{23}\\\\ a_{31} &amp; a_{32} &amp; a_{33}\\\\ \\end{bmatrix} \\begin{bmatrix} b_1\\\\ b_2\\\\ b_3\\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11}b_{1} + a_{12}b_{2} + a_{13}b_{3} \\\\ a_{21}b_{1} + a_{22}b_{2} + a_{23}b_{3}\\\\ a_{31}b_{1} + a_{32}b_{2} + a_{33}b_{3}\\\\ \\end{bmatrix} = a_{ij}b_{j}\\] convention convention Full expression $\\lambda = a_ib_i$ $\\lambda = \\sum\\limits_{i=1}^{3}a_ib_i$ $\\lambda = a_1b_1 + a_2b_2 + a_3b_3$ $c_i = S_{ik}x_k$ $c_i = \\sum\\limits_{i=1}^{3}S_{ik}x_k$ $c_i = \\begin{cases} c_1 = S_{11}x_1 + S_{12}x_2 + S_{13}x_3\\\\ c_2 = S_{21}x_1 + S_{22}x_2 + S_{23}x_3\\\\ c_3 = S_{31}x_1 + S_{32}x_2 + S_{33}x_3\\\\ \\end{cases}$ \\(\\lambda = S_{ij}S_{ij}\\) \\(\\lambda = \\sum\\limits_{j=1}^{3}\\sum\\limits_{i=1}^{3}S_{ij}S_{ij}\\) \\(\\lambda = S_{11}S_{11} + S_{12}S_{12} + \\cdots + S_{32}S_{32}+S_{33}S_{33}\\) \\(C_{ij} = A_{ik}B_{kj}\\) \\(\\lambda = \\sum\\limits_{k=1}^{3}A_{ik}B_{kj}\\) \\(\\big[C\\big]=\\big[A\\big]\\big[B\\big]\\) \\(C_{ij} = A_{ki}B_{kj}\\) \\(\\lambda = \\sum\\limits_{k=1}^{3}A_{ki}B_{kj}\\) \\(\\big[C\\big]=\\big[A\\big]^{T}\\big[B\\big]\\) ————————– ——————————————————————– —————————————————————————— : Uses of summation notation that students may encounter in the graduate core. Bracketed symbols indicate \\(3 \\times 3\\) matrices It will be important to learn how to read such summation notation, so if you see a repeated dummy index (often represented with \\(k\\) or \\(l\\), see Cai and Nix, 2.1.3), that you can recognize the notation. Some useful representations of summation notation are shown in Table 1: In future releases I will add summation notation for the Kronecker delta, \\(\\delta_{ij}\\), the LeviCivita \\(\\epsilon\\), the dot product, and the cross product, determinants, the del operator (\\(\\nabla\\)), and others as references. 3.4 Coordinate Transformations (Release 1/2017) Cartesian coordinates are not the only coordinate system that MSE graduate students will encounter in the core. Cylindrical coordinates and spherical coordinates are both useful in, for example, describing stress and strain fields around dislocations and vacancies. Cartesian coordinates, as mentioned in Sec. 1.2 utilize an orthogonal basis set and are often the easiest to use when describing and visualizing vector operations and physical laws. The rank-2 stress tensor (introduced in Sec. 1.2) is represented using the following \\(3 \\times 3 \\times 3\\) matrix and is shown in Fig.  2: Stress tensors for (a.) Cartesian, (b.) cylindrical, and (c.) spherical coordinate systems. From Nix and Cai. \\[\\sigma_{ij} \\begin{bmatrix} \\sigma_{xx} &amp; \\sigma_{xy} &amp; \\sigma_{xz}\\\\ \\sigma_{yx} &amp; \\sigma_{yy} &amp; \\sigma_{yz}\\\\ \\sigma_{zx} &amp; \\sigma_{zy} &amp; \\sigma_{zz}\\\\ \\end{bmatrix} \\label{eq:CartesianStressTensor}\\] Cylindrical coordinates are also an orthogonal coordinate system defined in Fig. 2(b). The stress tensor in this coordinate system is defined by the cylinderical components \\(r\\), \\(\\theta\\), and \\(z\\). Here, \\(r\\) is the distance from the \\(z\\)-axis to the point. \\(\\theta\\) is the angle between the reference direction (we use the \\(x\\)-direction) and the vector that points from the origin to the coordinates projected onto the \\(xy\\) plane. \\(z\\) is the distance from the point’s coordinates projected onto \\(xy\\) plane and the point itself. The stress tensor is represented as \\[\\sigma_{ij}= \\begin{bmatrix} \\sigma_{rr} &amp; \\sigma_{r \\theta} &amp; \\sigma_{r z}\\\\ \\sigma_{\\theta r} &amp; \\sigma_{\\theta\\theta} &amp; \\sigma_{\\theta z}\\\\ \\sigma_{z r} &amp; \\sigma_{z \\theta} &amp; \\sigma_{zz}\\\\ \\end{bmatrix} \\label{eq:CylindricalStressTensor}\\] Spherical coordinates are defined by \\(r\\), \\(\\theta\\) and \\(\\phi\\). Here \\(r\\) is the radial distance from the origin to the point. \\(\\theta\\) is the polar angle, or the angle between the \\(x\\)-axis and the point, projected onto the \\(xy\\) plane. \\(\\phi\\) is the azimuthal angle, or the angle between the \\(z\\)-axis and the vector pointing from the origin to the point. The stress tensor is \\[\\sigma_{ij}= \\begin{bmatrix} \\sigma_{rr} &amp; \\sigma_{r \\theta} &amp; \\sigma_{r \\phi}\\\\ \\sigma_{\\theta r} &amp; \\sigma_{\\theta\\theta} &amp; \\sigma_{\\theta \\phi}\\\\ \\sigma_{\\phi r} &amp; \\sigma_{\\phi \\theta} &amp; \\sigma_{\\phi\\phi}\\\\ \\end{bmatrix} \\label{eq:SphericalStressTensor}\\] We will often want to transform tensor values from one coordinate system to another in \\({\\rm I\\!R}^3\\). As an example, we will convert the stress state from a cylinderical coordinate system to a Cartesian coordinate system. This transformation from stress state in the original coordinate system (\\(\\sigma_{kl } = \\sigma_{kl}^{r \\theta z}\\)) to the new coordinate system (\\(\\sigma_{ij }^{&#39;} = \\sigma_{ij}^{xyz}\\)) is performed using the following relationship: \\[\\sigma_{ij}&#39; = Q_{ik}Q_{jk}\\sigma_{kl} \\label{eq:GeneralTransform}\\] Where the summation notation (Sec. 1.3) is implicit. In our example the indices \\(kl\\) indicate the original cylindrical coordinate system (\\(r\\), \\(\\theta\\), \\(z\\)) and the indices \\(ij\\) indicate the new coordinate system (\\(x\\), \\(y\\), \\(z\\)). Note that Eq. \\[eq:GeneralTransform\\] can be written in matrix form as: \\[\\sigma = Q \\cdot \\sigma \\cdot Q^{T}\\] The \\(Q\\) matrix is defined the dot products between the unit vectors in the coordinate systems of interest. In simplified 2D transformation from polar to Cartesian, there is no \\(z\\) component in either coordinate system and terms with those indices can be dropped. \\[Q_{ik} \\equiv (\\hat{e}_{i}^{xy} \\cdot \\hat{e}_k^{r \\theta}) = \\begin{bmatrix} (\\hat{e}_{x} \\cdot \\hat{e}_{r}) &amp; (\\hat{e}_{x} \\cdot \\hat{e}_{\\theta})\\\\ (\\hat{e}_{y} \\cdot \\hat{e}_{r}) &amp; (\\hat{e}_{y} \\cdot \\hat{e}_{\\theta})\\\\ \\end{bmatrix}\\\\ %Q_{jl} \\equiv (\\hat{e}_{j}^{xyz} \\cdot \\hat{e}_l^{r \\theta z}) \\\\\\] where \\(\\hat{e}_{r}\\) and \\(\\hat{e}_{\\theta}\\) is related geometrically to \\(\\hat{e}_{x}\\) and \\(\\hat{e}_{y}\\): \\[\\begin{bmatrix} \\hat{e}_{r} = \\hat{e}_{x} \\cos(\\theta) + \\hat{e}_{y} \\sin(\\theta)\\\\ \\hat{e}_{\\theta} = -\\hat{e}_{x} \\sin(\\theta) + \\hat{e}_{y} \\cos(\\theta)\\\\ \\end{bmatrix}\\\\ %Q_{jl} \\equiv (\\hat{e}_{j}^{xyz} \\cdot \\hat{e}_l^{r \\theta z}) \\\\\\] And therefore: \\[\\begin{aligned} Q_{ik} &amp;\\equiv (\\hat{e}_{i}^{xy} \\cdot \\hat{e}_k^{r \\theta}) = \\begin{bmatrix} (\\hat{e}_{x} \\cdot \\hat{e}_{r}) &amp; (\\hat{e}_{x} \\cdot \\hat{e}_{\\theta})\\\\ (\\hat{e}_{y} \\cdot \\hat{e}_{r}) &amp; (\\hat{e}_{y} \\cdot \\hat{e}_{\\theta})\\\\ \\end{bmatrix} = \\begin{bmatrix} Q_{xr} &amp; Q_{x\\theta}\\\\ Q_{yr} &amp; Q_{y\\theta}\\\\ \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\left(\\hat{e}_{x} \\cdot \\left[\\hat{e}_{x} \\cos(\\theta) + \\hat{e}_{y} \\sin(\\theta)\\right]\\right) &amp; \\left(\\hat{e}_{x} \\cdot \\left[-\\hat{e}_{x} \\sin(\\theta) + \\hat{e}_{y} \\cos(\\theta)\\right]\\right)\\\\ \\left(\\hat{e}_{y} \\cdot \\left[\\hat{e}_{x} \\cos(\\theta) + \\hat{e}_{y} \\sin(\\theta)\\right]\\right) &amp; \\left(\\hat{e}_{y} \\cdot \\left[-\\hat{e}_{x} \\sin(\\theta) + \\hat{e}_{y} \\cos(\\theta)\\right]\\right) \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} \\cos(\\theta) &amp; -\\sin(\\theta)\\\\ \\sin(\\theta) &amp; \\cos(\\theta) \\end{bmatrix}\\end{aligned}\\] So, to convert the stress tensor in polar coordinates (\\(\\sigma_{kl}^{r\\theta}\\)) to Cartesian (\\(\\sigma_{ij}^{xy}\\)), we take the triple dot-product: \\[\\begin{aligned} \\sigma&#39; &amp;= Q \\cdot \\sigma \\cdot Q^{T} = \\begin{bmatrix} \\sigma_{xx} &amp; \\sigma_{xy}\\\\ \\sigma_{yx} &amp; \\sigma_{yy} \\end{bmatrix}= \\begin{bmatrix} \\cos(\\theta) &amp; -\\sin(\\theta)\\\\ \\sin(\\theta) &amp; con(\\theta) \\end{bmatrix}\\cdot \\begin{bmatrix} \\sigma_{rr} &amp; \\sigma_{r\\theta}\\\\ \\sigma_{\\theta r} &amp; \\sigma_{\\theta \\theta} \\end{bmatrix} \\cdot \\begin{bmatrix} \\cos(\\theta) &amp; \\sin(\\theta)\\\\ -\\sin(\\theta) &amp; con(\\theta) \\end{bmatrix} %&amp;= %\\begin{bmatrix} %\\cos(\\theta) (\\sigma_{rr} \\cos(\\theta) - \\sigma_{\\theta r} \\sin(\\ %\\theta)) - \\sin(\\theta)(\\sigma_{r\\theta}\\cos(\\theta) - \\sigma_{\\theta\\theta}\\sin(\\theta) &amp; \\sin(\\theta) (\\sigma_{rr} \\cos(\\theta) - \\sigma_{\\theta r} \\sin(\\ %\\theta)) + \\cos(\\theta)(\\sigma_{r\\theta}\\cos(\\theta) - \\sigma_{\\theta\\theta}\\sin(\\theta)\\\\ %\\cos(\\theta) (\\sigma_{\\theta r} \\cos(\\theta) + \\sigma_{rr} \\sin(\\ %\\theta)) - \\sin(\\theta)(\\sigma_{\\theta \\theta}\\cos(\\theta) + \\sigma_{r\\theta}\\sin(\\theta) &amp; \\sin(\\theta) (\\sigma_{\\theta r} \\cos(\\theta) + \\sigma_{rr} \\sin(\\ %\\theta)) + \\cos(\\theta)(\\sigma_{\\theta \\theta}\\cos(\\theta) + \\sigma_{r\\theta}\\sin(\\theta) %\\end{bmatrix}\\end{aligned}\\] Completing the math yields: \\[\\begin{aligned} \\sigma_{xx} &amp;= \\cos(\\theta) \\left[\\sigma_{rr} \\cos(\\theta) - \\sigma_{\\theta r} \\sin( \\theta)\\right] - \\sin(\\theta)\\left[\\sigma_{r\\theta}\\cos(\\theta) - \\sigma_{\\theta\\theta}\\sin(\\theta)\\right]\\\\ \\sigma_{xy} &amp;= \\sin(\\theta) \\left[\\sigma_{rr} \\cos(\\theta) - \\sigma_{\\theta r} \\sin( \\theta)\\right] + \\cos(\\theta)\\left[\\sigma_{r\\theta}\\cos(\\theta) - \\sigma_{\\theta\\theta}\\sin(\\theta)\\right]\\\\ \\sigma_{yx} &amp;= \\cos(\\theta) \\left[\\sigma_{\\theta r} \\cos(\\theta) + \\sigma_{rr} \\sin( \\theta)\\right] - \\sin(\\theta)\\left[\\sigma_{\\theta \\theta}\\cos(\\theta) + \\sigma_{r\\theta}\\sin(\\theta)\\right]\\\\ \\sigma_{yy} &amp;= \\sin(\\theta) \\left[\\sigma_{\\theta r} \\cos(\\theta) + \\sigma_{rr} \\sin( \\theta)\\right] + \\cos(\\theta)\\left[\\sigma_{\\theta \\theta}\\cos(\\theta) + \\sigma_{r\\theta}\\sin(\\theta)\\right]\\end{aligned}\\] In as system with only one or two stress components these coordinate transformations simplify greatly. Remember, though, in \\({\\rm I\\!R}^3\\) there will be \\(N = 3^2\\) components due to increased dimentionality. 3.5 Calculus We assume that incoming graduate students have completed coursework in calculus including the basic calculation of derivatives, antiderivatives, definite integrals, series/sequences, and multivariate calculus. Below are outlined some more advanced calculus concepts that have specific physical relevance to concepts covered in the MSE core. Any college-level calculus text is suitable for supplemental study. The sections below on Total Differentials (Sec. 1.5.1) and Exact/Inexact Differentials (Sec. 1.5.2) were adapted from the course materials of Richard Fitzpatrick at UT-Austin (available here). 3.5.1 Total Differentials: (Release 11/2016) Encountered in: MAT_SCI 401 When there exists an explicit function of several variables such as \\(f = f(x,y,t)\\), which has \\(f\\) has a total differential of form: \\[\\begin{aligned} \\Diff{}{f} = \\Big(\\Partial{}{f}{t}\\Big)_{x,y}\\Diff{}{t} + \\Big(\\Partial{}{f}{x}\\Big)_{t,y} \\Diff{}{x} + \\Big(\\Partial{}{f}{y}\\Big)_{t,x} \\Diff{}{y}\\end{aligned}\\] Here, we do not assume that \\(f\\) is constant with respect any of the arguments \\((x\\text{,}\\, y\\text{, or } t)\\). This equation defines the differential change in the function \\(\\Diff{}{f}\\) and accounts for all interdependencies between \\(x\\), \\(y\\), and \\(t\\). In general, the total differential can be defined as: \\[\\begin{aligned} \\label{eq:TotDiff} \\Diff{}{f} = \\sum\\limits_{i=1}^n \\Big(\\Partial{}{f}{x_i}\\Big)_{x_{j\\neq i}}\\Diff{}{x_i}\\end{aligned}\\] The total differential is important when working with thermodynamic systems which is described by thermodynamic parameters (e.g. \\(P\\), \\(T\\), \\(V\\)) which are not necessary independent. For example, the internal energy \\(U\\) for some homogeneous system can be defined in terms of entropy \\(S\\) and volume \\(V\\); \\(U = U(S,V)\\). According to Eq. \\[eq:TotDiff\\], the infinitesimal change in internal entropy is therefore: \\[\\begin{aligned} \\Diff{}{U} = \\Big(\\Partial{}{U}{S}\\Big)_{V}\\Diff{}{S} + \\Big(\\Partial{}{U}{V}\\Big)_{S} \\Diff{}{V}\\end{aligned}\\] 3.5.2 Exact and Inexact Differentials (Release 11/2016) Encountered in: MAT_SCI 401 Suppose we are assessing the infinitesimal change of some value: \\(\\Diff{}{f}\\), in which \\(\\Diff{}{f}\\) is a linear differential of the form: \\[\\begin{aligned} \\Diff{}{f} = \\sum\\limits_{i=1}^m M_i(x_1,x_2,...x_m)\\Diff{}{x_i}.\\end{aligned}\\] In thermodynamics we are often concerned with linear differentials of two independent variables such that \\[\\begin{aligned} \\label{eq:LinearDiff} \\Diff{}{f} = M(x,y) \\Diff{}{x} + N(x,y) \\Diff{}{y}.\\end{aligned}\\] An exact differential is one in which \\(\\int{\\Diff{}{z}}\\) is path-independent. It can be shown (e.g. Wolfram Exact Differential) that this means: \\[\\begin{aligned} \\label{eq:ExactDiff} \\Diff{}{f} = \\Big(\\Partial{}{f}{x}\\Big)_{y} \\Diff{}{x} + \\Big(\\Partial{}{f}{y}\\Big)_{x} \\Diff{}{y}.\\end{aligned}\\] Which means that \\[\\begin{aligned} \\label{eq:ExactDiff2} \\Big(\\Partial{}{M}{y}\\Big)_{x} = \\Big(\\Partial{}{N}{x}\\Big)_{y}. %Wolfram proof.\\end{aligned}\\] An inexact differential is one in which the equality defined in Eq. \\[eq:ExactDiff\\] (and therefore Eq. \\[eq:ExactDiff2\\]) is not necessary true. An inexact differential is typically denoted using bar notation to define the infinitesimal value: \\[\\begin{aligned} \\text{\\dj} f = \\Big(\\Partial{}{f}{x}\\Big)_{y} \\Diff{}{x} + \\Big(\\Partial{}{f}{y}\\Big)_{x} \\Diff{}{y}.\\end{aligned}\\] Two physical examples make this more clear: Example 1: Imagine you are speaking with a classmate who recently traveled from from Chicago to Minneapolis. You know he is now in Minneapolis. Is it possible for you to know how much money he spent gas (\\(G\\))? No, you can’t. \\(G\\) is dependent on how your friend traveled to Minneapolis: his gas mileage, the cost of gas, and, of course, the route he took. \\(G\\) cannot be known without understanding the details of the path, and is therefore not path independent. The differenitial of \\(G\\) is therefore inexact: \\(G\\). Now, what do we know about your friend’s distance, \\(D\\), to Chicago? This value does not dependent on how he traveled, the only information you need to know is his location, now, in Minneapolis. His distance to Chicago, therefore is a state variable and \\(\\Diff{}{D}\\) is an exact differential. Example 2: Let’s reconsider a situation like that of Example 1 this within the purview of thermodynamics. Consider the internal energy \\(U\\) of a closed system. To achieve an infinitesimal change in energy \\(\\Diff{}{U}\\), we provided a bit of work \\(\\text{\\dj}W\\) or heat \\(\\text{\\dj}Q\\): \\(\\Diff{}{U} = \\text{\\dj}W + \\text{\\dj}Q\\).1 The work performed and heat exchanged on the system is path-dependent — the total work done depends on how the work was performed or heat exchanged, and so \\(\\text{\\dj}W\\) and \\(\\text{\\dj}Q\\) are inexact. It is sometimes useful to ask yourself about the nature of a variable to ascertain whether the differential is exact or inexact. That is, it makes sense to ask yourself: “what is the energy of the system?” or “what is the pressure of the system”? This often helps in the identification of a state variable. However, it does not make sense to ask yourself “what is the work of the system” or “what is the heat” of the system — these values depend on the process. Instead, you have to ask yourself: “what is the work done on the system along this path?” or “what is the heat exchanged during this process?”. Finally, there are different properties we encounter during the evaluation exact differential (such as the linear differential in Eq. \\[eq:LinearDiff\\]), and inexact differentials (written as \\(\\text{\\dj}f = M&#39;(x,y) \\Diff{}{x} + N&#39;(x,y) \\Diff{}{y}\\)). The integral of an exact differential over a closed path is necessary zero: \\[\\begin{aligned} \\oint\\Diff{}{f} \\equiv 0,\\end{aligned}\\] while the integral of an inexact differential over a closed path is not necessarily zero: \\[\\begin{aligned} \\oint\\text{\\dj}f\\underset{n}{\\neq} 0.\\end{aligned}\\] where \\(\\Big(\\underset{n}{\\neq}\\Big)\\) symbolizes “not necessarily equal to”. Indeed, when evaluating the inexact differential, it is important to consider the path. For example, the work performed a system going from a macrostate \\(s_i\\) to a macrostate \\(s_2\\) is defined by path \\(L_{1}\\), then the total work performed is defined: \\[\\begin{aligned} W_{L_{1}} = \\int\\limits_{L_{1}} \\text{\\dj}W\\end{aligned}\\] If we took a different path, \\(L_{2}\\), the total work performed by be different and \\[\\begin{aligned} W_{L_{1}} \\underset{n}{\\neq} W_{L_{2}}\\end{aligned}\\] A good illustration of a line integral over a scalar field is shown in the multimedia Fig. \\[fig:LineIntegral\\]. It is clear that, depending on the path, the evaluated integral will have different values. 3.5.3 Vector Calculus (Release TBD) Encountered in: MAT_SCI 406, 408 3.6 Differential Equations Differential equations — equations that relate functions with their derivatives — are central to the description of natural phenomena in physics, chemistry, biology and engineering. In the sections below, we will outline basic classification of differential equations and describe methods and techniques used in solving equations that are encountered in the MSE graduate core. The information provide below is distilled and specific to the MSE core, but is by no means a equivalent to a thorough 1- or 2-quarter course in ODEs and PDEs. For students who are completely unfamiliar with the material below; i.e., those who have not taken a course in differential equations, we highly recommend enrollment in Applied Math 311-1 and 311-2.2 3.6.1 Classification of Differential Equations (Release: 11/2016) Encountered in: MAT_SCI 405, 406, 408 Classification of differential equations provide intuition about the physical process that the equation describes, as well as providing context we use as we go about solving the equation. A differential equation can be classified as either ordinary or partial, linear or non-linear, and by its homogeneity and equation order. These are described briefly below, with examples. 3.6.1.1 Ordinary and Partial Differential Equations — The primary classification we use to organize types of differential equations is whether they are ordinary or partial differential equations. Ordinary differential equations (ODEs) involve functions of a single variable. All derivatives present in the ODE are relative to that one variable. Partial differential equations are functions of more than one variable and the partial derivatives of these functions are taken with respect to those variables. An example of an ODE is shown in Eq. \\[eq:RLC\\]. This equation has two functions \\(q(t)\\) (charge) and \\(V(t)\\) (voltage), the values of which depend on time \\(t\\). All of the derivatives are with respect the independent variable \\(t\\). \\(L\\), \\(R\\), and \\(C\\) are constants. \\[\\begin{aligned} L \\FullDiff{2}{q(t)}{t} + R \\FullDiff{}{q(t)}{t} + \\frac{1}{C} q(t) = V(t) \\label{eq:RLC}\\end{aligned}\\] This general example describes the flow of charge as a function of time in a RLC circuit with an applied voltage that changes with time. Other examples of ODEs you may encounter in the MSE core include ODEs for grain growth as a function of time and the equations of motion. Partial differential equations (PDEs) contain multivariable functions and their partial derivatives i.e., a derivative with respect to one variable with others held constant. As physical phenomenon often vary in both space and time, PDEs — and methods of solving them — will be encountered in many of the core MSE courses. These phenomena include wave behavior, diffusion, the Schödinger equation, heat conduction, the Cahn-Hilliard equation, and many others. A typical example of a PDEs you will encounter is Fick’s Second Law. In 1D, this is: \\[\\begin{aligned} \\Partial{}{\\varphi(x,t)}{t} = D\\Partial{2}{\\varphi(x,t)}{x} \\label{eq:Ficks2}\\end{aligned}\\] where \\(\\varphi\\) is the concentration as a function of position \\(x\\) and time \\(t\\). This expression equates the change in the concentration over time to the shape (concavity) of the concentration profile. Partial differential equations are, by nature, often more difficult to solve than ODEs, but, as with ODEs, there exist simple, analytic, and systematic methods for solving many of these equations. 3.6.1.2 Equation Order — The order of a differential equation is simply the order of the highest derivative that is present in the equation. In the preceding section, Eq. \\[eq:RLC\\] is a second-order equation. Eq. \\[eq:Ficks2\\] is also a second-order equation. Students in the MSE core will encounter 4th-order equations such as the Cahn-Hilliard equation, which describes phase separation and is discussed in detail in MAT_SCI 408. One note concerning notation — when writing higher-order differential equations it is common to abandon Leibniz’s notation (where an \\(n^{\\text{th}}\\)-order derivative is denoted as \\(\\FullDiff{n}{f}{x}\\)) in favor of Lagrange’s notation in which the following representations are equivalent: \\[\\begin{aligned} \\text{Leibniz}:&amp; F\\big[x,f(x),\\FullDiff{}{f(t)}{x},\\FullDiff{2}{f(t)}{x}...\\FullDiff{n}{f(t)}{x}\\big] = 0 \\rightarrow\\\\ \\text{Lagrange}:&amp; F\\big[x,f,f\\prime,f\\prime\\prime...f^{(n)}\\big] = 0 \\label{eq:LagrangeNote}\\end{aligned}\\] An example would be be the 3rd-order differential equation: \\[\\begin{aligned} f\\prime\\prime\\prime + 3f\\prime + f\\exp{x} = x\\end{aligned}\\] 3.6.1.3 Linearity — While considering how to solve a differential equation, it is crucial to consider whether an equation is linear or non-linear. For example, an ODE like that represented in Eq. \\[eq:LagrangeNote\\] is linear if the \\(F\\) is a linear function of the variables \\(f, f&#39;, f\\prime\\prime...f^{(n)}\\). This definition also applies to PDEs. The expression for the general linear ODE of order \\(n\\) is: \\[\\begin{aligned} a_0(x)f^{(n)}+a_1(x)f^{(n-1)} + ... + a_n(x)f = g(t) \\label{eq:LinearODE}\\end{aligned}\\] Any expression that is not of this form is considered nonlinear. The presence of a product such as \\(f\\cdot f\\prime\\), a power such as \\((f\\prime)^2\\), or a sinusoidal function of \\(f\\) would make the equation nonlinear. The methods of solving linear differential equations are well-developed. Nonlinear differential equations, on the other hand, often require more complex analysis. As you will see, methods of linearization (small-angle approximations, stability theory) as well as numerical techniques are powerful ways to approach these problems. 3.6.1.4 Homogeneity — Homogeneity of a linear differential equation, such as that shown in Eq. \\[eq:LinearODE\\] is satisfied if \\(g(x) = 0\\). This property of a differential equation is often connected to the driving force in a system. For example, the motion of a damped harmonic oscillator in 1D (derived from Newton’s laws of motion, here) is described by a homogeneous linear, 2nd-order ODE: \\[\\begin{aligned} x\\prime\\prime+2\\zeta \\omega_0 x\\prime \\omega_0^2 x = 0\\end{aligned}\\] where \\(x = x(t)\\) is position as a function of time (\\(t\\)) , \\(\\omega_0\\) is the undamped angular frequency of the oscillator, and \\(\\zeta\\) is the damping ratio. If we add a sinusoidal driving force, however, the equation becomes inhomogeneous: \\[\\begin{aligned} x\\prime\\prime+2\\zeta \\omega_0 x\\prime + \\omega_0^2 x = \\frac{1}{m} F_0 \\sin{(\\omega t)} \\label{eq:DDSOscillator}\\end{aligned}\\] One may notice that the form for Eq. \\[eq:DDSOscillator\\] is exactly that of the first equation shown in this section (Eq. \\[eq:RLC\\]) — the ODE for a damped, driven harmonic oscillator is exactly the same form as that of the RLC circuit operating under a alternating driving voltage. 3.6.1.5 Boundary Conditions — Differential equations, when combined with a set of well-posed constraints, or boundary conditions, define a boundary value problem. Well-posed boundary value problems have unique solutions from the imposed physical constraints on the system of interest. This analysis allows for the extraction of relevant physical information investigating a physical system — the elemental composition at some position at time within a diffusion couple, the equilibrium displacement in a mechanically deformed body, or the energy eigenstate of a quantum system. While boundary conditions are not used not classify a differential equation itself, boundary conditions are used to classify the entire boundary value problem — which is defined by both the differential equation and the boundary value conditions. Boundary value problems are at the heart of physical description in science and engineering. Solving these types of problems allow for the extraction of information (concentration, deformation, stress state, quantum state, etc.) from a system. There are a few types of boundary conditions that you may encounter in the MSE core: A Dirichlet (or first-type) boundary condition is one in which specific values are fixed on the boundary of a domain. An example of this is a system in which we have diffusion of carbon (in, for example, a carbourizing atmosphere) into iron (possessing a volume defined as domain \\(\\Omega\\)) where the carbon concentration \\(C(\\mathbf{r},t)\\) at the interface is known for all time \\(t &gt; 0\\). Here, \\(\\mathbf{r}\\) is position vector and the domain boundary is denoted as \\(\\partial \\Omega\\)). If this concentration is a known function, \\(f(\\textbf{r},t)\\), then the Dirichlet condition is described as: \\[C(\\textbf{r},t) = f(\\textbf{r},t), \\quad \\forall\\textbf{r} \\in \\partial \\Omega\\] A Neumann (or second-type) boundary condition is the values of the normal derivative (a directional derivative with respect to the normal of a surface or boundary represented by the vector \\(\\mathbf{n}\\)) of the solution are known at the domain boundary. Continuing with our example above, this would mean we know the diffusion flux normal to the the boundary at \\(r\\) at all times \\(t\\): \\[\\Partial{}{C(\\textbf{r},t)}{\\mathbf{n}} = g(\\textbf{r},t), \\quad \\forall\\textbf{r} \\in \\partial \\Omega\\] where \\(g(\\textbf{r},t)\\) is a known function, and the bold typesetting denotes a vector. Two other types of boundary conditions you may encounter are Cauchy and Robin. Cauchy boundary conditions specifies both the solution value and its normal derivative at the boundary — i.e., it provides both Dirichlet and Neumann conditions. The Robin condition provides a linear combination of the solution and its normal derivative and is common in convection-diffusion equations. Periodic boundary conditions are applied in periodic media or large, ordered systems. Previously described boundary conditions can therefore combined into periodic sets using infinite sums of sine and cosine functions to create Fourier series. This will be discussed in more detail in Sec. 1.6.2.4. 3.6.2 Solving Differential Equations (Release: 11/2016) There are many ways to solve differential equations, including analytical and computational techniques. Below, we outline a number of methods that are used in the MSE core to solve relevant differential equations. 3.6.2.1 Separation of Variables , also known as the Fourier Method, is a general method used in both ODEs and PDEs to reconstruct a differential equation so that the two variables are separated to opposite sides of the equation and then solved using techniques covered in an ODE class. \\[sec:SepVar\\] This method will be used in the solving of many simpler differential equations such as the heat and diffusion equations. These equations must be linear and homogeneous for separation of variables to work. The main goal is to take some sort of differential equation, for example an ordinary differential equation: \\[\\begin{aligned} \\FullDiff{}{y}{x} &amp;= g(x)h(y)\\\\ \\intertext{which we can rearrange as:} \\frac{1}{h(y)}\\mathop{dy} &amp;= g(x)dx\\\\ \\intertext{We now integrate both sides of the equation to find the solution:} \\int{\\frac{1}{h(y)}\\mathop{dy}} &amp;= \\int{g(x)\\mathop{dx}}\\end{aligned}\\] Clearly, we have separated our two variables, \\(x\\) and \\(y\\), to opposite sides of the equation. If the functions are integrable and the resulting integration can be solved for \\(y\\), then a solution can be obtained. Note here that we have treated the \\(\\mathop{dy}/\\mathop{dx}\\) derivative as a fraction which we have separated. Example 1: Exponential growth behavior can be represented by the equation: \\[\\begin{aligned} \\FullDiff{}{y(t)}{t} &amp;= k y(t)\\\\ \\intertext{or} \\FullDiff{}{y}{t} &amp;= k y\\\\ \\intertext{This expression simply states that the growth rate of some quantity $y$ at time, $t$, is proportional to the value of $y$ itself at that time. This is a seperable equation:} \\frac{1}{y}dy &amp;= k dt\\\\ \\intertext{We can integrated both sides to get:} \\int{\\frac{1}{y}dy} &amp;= k \\int{dt}\\\\ \\text{ln}(y)+C_1 &amp;= k t + C_2\\\\ \\intertext{where $C_1$ and $C_2$ are the constants of integration. These can be combined:} \\text{ln}(y) &amp;= kt+\\tilde{C}\\\\ y &amp;= e^{(kt+\\tilde{C})}\\\\ y &amp;= Ce^{kt} \\end{aligned}\\] This is clear exponential growth behavior as a function of time. Separation of variables is extremely useful in solving various ODEs and PDEs — it is employed in the solving of the diffusion equation in . 3.6.2.2 Sturm-Liouville Boundary Value Problems In this section, we use Sturm-Liouville theory in solving a separable, linear, second-order homogeneous partial differential equation. Sturm-Liouville theory can be used on differential equations (here, in 1D) of the form: \\[\\begin{aligned} \\FullDiff{}{}{x}\\Big[p(x)\\FullDiff{}{y}{x}\\Big]-q(x)y+\\lambda r(x)y = 0 \\label{eq:SturmLiouville} \\intertext{or} \\big[p(x)y\\prime]\\prime-q(x)y+\\lambda r(x)y = 0 \\label{eq:SturmLiouville-2}\\end{aligned}\\] This type of problem requires knowledge of many use of many concepts and techniques in solving ODEs, including , Fundamental Solutions of Linear First- and Second-Order Homogeneous Equations, Fourier Series, and Orthogonal Solution Functions. It is important to note that the approach described below (adapted from JJ Hoyt’s Phase Transformations), which employs separation of variables and Fourier transforms, works only on linear equations. A different approach must be taken for non-linear equations (such as Cahn-Hilliard). We will use the example of a solid slab of material of length \\(L\\) that has a constant concentration of some elemental species at time zero \\(\\varphi(x,0) = \\varphi_0\\) for all \\(x\\) within the slab. On either end of the slab we have homogeneous boundary conditions defining the surface concentrations fixed at \\(\\varphi(0,t) = \\varphi(L,t) = 0\\) for all \\(t\\). The changing concentration profile, \\(\\varphi(x,t)\\) is dictated by Fick’s second law, as described earlier in Eq. \\[eq:Ficks2\\]: \\[\\Partial{}{\\varphi(x,t)}{t} = D\\Partial{2}{\\varphi(x,t)}{x} \\label{eq:Ficks2-1}\\] To use separation of variables, we define the concentration \\(\\varphi(x,t)\\), which is dependent on both position and time, to be a product of two functions, \\(T(t)\\) and \\(X(x)\\): \\[\\begin{aligned} \\varphi(x,t) &amp;= T(t)X(x) \\label{eq:SepVar-1} \\intertext{or, in shorthand,} \\varphi &amp;= TX\\end{aligned}\\] It isn’t clear why we do this at this point, but stay tuned. Combining Eqs. \\[eq:Ficks2-1\\] and \\[eq:SepVar-1\\] yields: \\[XT\\prime = DTX\\prime\\prime\\] Where the primed Lagrange notation denotes total derivatives. \\(T\\) and \\(X\\) are functions only of \\(t\\) and \\(x\\), respectively. Now, we separate the variables completely to acquire: \\[\\frac{1}{DT}T\\prime = \\frac{1}{X}X\\prime\\prime\\] This representation conveys something critical: each side of the equation must be equal to the same constant. This is because the two sides of the equation are equal to each other and the only way a collection of time-dependent quantities can be equivilent to a selection of position-dependent quantities is for them to be constant with respect to both time and position. We select this constant — for reasons that become clear of the convience of this selection later in the analysis — as \\(-\\lambda^2\\): \\[\\begin{aligned} \\frac{1}{DT}T\\prime &amp;= -\\lambda^2 \\label{eq:SepT}\\\\ \\frac{1}{X}X\\prime\\prime &amp;= -\\lambda^2 \\label{eq:SepX} \\end{aligned}\\] Integration of Eq. \\[eq:SepT\\] yields, from : \\[\\begin{aligned} \\frac{1}{DT}T\\prime &amp;= -\\lambda^2 \\nonumber\\\\ \\frac{1}{T}\\FullDiff{}{T}{t} &amp;= -\\lambda^2 D \\nonumber\\\\ \\int \\frac{1}{T}\\Diff{}{T} &amp;= -\\int \\lambda^2 D \\Diff{}{t} \\nonumber\\\\ \\ln{T} &amp;= -\\lambda^2 D t + T_0 \\nonumber\\\\ \\intertext{where $T_0$ is the combined constant of integration:} T = T(t) &amp;= \\exp{(-\\lambda^2 D t + T_0)} \\nonumber\\\\ T(t) &amp;= T_0 \\exp{(-\\lambda^2 D t)} \\label{eq:Tt}\\end{aligned}\\] Eq. \\[eq:SepX\\], on the other hand, is a linear, homogeneous, second-order ODE with constant coefficients that describes simple harmonic behavior. We can solve this by assessing its characteristic equation: \\[\\begin{aligned} r^2+\\lambda^2 = 0\\\\ \\intertext{which has roots:} r = \\pm \\lambda i\\end{aligned}\\] When the roots of the characteristic equation are of the form \\(r = \\alpha \\pm \\beta i\\), the solution of the differential equation (Pg. 5) is: \\[y = e^{\\alpha x}(c_1 \\cos{\\beta x} + c_2 \\sin{\\beta x})\\] In this instance, \\(\\alpha = 0\\) and \\(\\beta = \\lambda\\), so our solution is: \\[X = X(x) = \\tilde{A} \\cos{\\lambda x} + \\tilde{B} \\sin{\\lambda x} \\label{eq:Xx}\\] \\(\\tilde{A}\\) and \\(\\tilde{B}\\) are constants that will be further simplified later. Recalling Eq. \\[eq:SepVar-1\\] and utilizing our results from Eqs. \\[eq:Tt\\] and \\[eq:Xx\\], we find: \\[\\begin{aligned} \\varphi(x,y) &amp;= X(x)T(x) = T_0 \\big[\\tilde{A} \\cos{\\lambda x} + \\tilde{B} \\cos{\\lambda x}\\big]\\exp{(-\\lambda^2 D t)}\\\\ \\intertext{where we now define $T_0 \\tilde{A} = A$ and $T_0 \\tilde{B} = B$ to get:} \\varphi(x,y) &amp;= X(x)T(x) = \\big[A\\cos{\\lambda x} + B\\sin{\\lambda x}\\big]\\exp{(-\\lambda^2 D t)} \\label{eq:DiffSol}\\end{aligned}\\] Physially, this solution begins to make sense. At \\(t=0\\) we have a constant concentration, but concentration begins to decay esponentially with time as \\(D\\), \\(t\\), and \\(\\lambda\\) are all positive, real constants. The concentration profile is a linear combination of sine and cosine functions, which does not yet yield any physical intuition for this system as we have yet to utilize boundary conditions. Recall at this point that we have not specified any value for the constant \\(\\lambda\\), as is typical when solving this type of Sturm-Liouville problem. This suggests that there are possible solutions for all values of \\(\\lambda_n\\). The Principle of Superposition dictates, then, that if Eq. \\[eq:DiffSol\\] is a solution, the complete solution to the problem is a summation of all possible solutions: \\[\\begin{aligned} \\Aboxed{\\varphi(x,y) = \\sum_{n=1}^\\infty \\big[A_n\\cos{\\lambda_n x} + B_n\\sin{\\lambda_n x}\\big]\\exp{(-\\lambda_n^2 D t)}} \\label{eq:DiffSolFull}\\end{aligned}\\] As the value of \\(\\lambda\\) influences the values of \\(A\\) and \\(B\\), these values must also be calculated for each \\(\\lambda_n\\). Now, to completely solve our well-posed boundary value problem, we utilize our boundary conditions: \\[\\begin{aligned} \\varphi(0,t) &amp;= 0\\, \\quad t \\geq 0 \\label{eq:Boundx0}\\\\ \\varphi(L,t) &amp;= 0\\, \\quad t \\geq 0 \\label{eq:BoundxL}\\\\ \\varphi(x,0) &amp;= \\varphi_0\\, \\quad 0&lt;x&lt;L \\label{eq:Time0} \\end{aligned}\\] At \\(x = 0\\), the sine term in Eq. \\[eq:DiffSolFull\\] is zero, and therefore the boundary condition in Eq. \\[eq:Boundx0\\] can only be satisfied at all t if \\(A_n = 0\\). At \\(x = L\\), \\(\\sin{\\lambda_n x}\\) must be zero for all values of \\(\\lambda_n\\), therefore \\(\\lambda_n = n\\pi/L\\). We need only solve now for \\(B_n\\) using the intial condition, Eq. \\[eq:Time0\\]. Using our values of \\(A_n\\) and \\(\\lambda_n\\) and assessing Eq. \\[eq:DiffSolFull\\] at time \\(t=0\\) yields \\[\\varphi_0 = \\sum_{n=1}^\\infty B_n \\sin{\\frac{n \\pi x}{L}} \\label{eq:Time0-1}\\] Here, we must recognized the orthogonal property of the sine function, which states that \\[\\int_0^L \\sin{\\frac{n \\pi x}{L}} \\sin{\\frac{m \\pi x}{L}} \\begin{cases} = 0, &amp; \\text{if}\\ n\\neq m \\\\ \\neq 0, &amp; \\text{if}\\ n = m \\end{cases}\\] You can test this graphically using a plotting program if you like — the integrated value of this product is only non-zero when \\(n=m\\) — or you can follow the proof here. We can multiply both sides of the Eq. \\[eq:Time0-1\\] by \\(\\sin{n \\pi x/L}\\), then, and integrate both sides from 0 to \\(L\\): \\[\\begin{aligned} \\varphi_0 \\int_0^L\\sin{\\frac{m \\pi x}{L}} &amp;= \\int_0^L \\sum_{n=1}^\\infty \\big[B_n \\sin{\\frac{n \\pi x}{L}} \\sin{\\frac{m \\pi x}{L}}\\big] \\nonumber \\intertext{After integration, the only term that survives on the right-hand side is the $m=n$ term, and therefore:} \\varphi_0 \\int_0^L\\sin{\\frac{n \\pi x}{L}} &amp;= B_n\\int_0^L \\sin{\\frac{n \\pi x}{L}}^2 \\nonumber\\\\ \\varphi_0 \\int_0^L\\sin{\\frac{n \\pi x}{L}} &amp;= \\frac{B_n L}{4} \\big[2- \\frac{\\sin{2 n \\pi}}{n \\pi} \\big] \\nonumber\\\\ \\intertext{the $\\sin{2 n \\pi}$ term is always zero:} \\varphi_0 \\int_0^L\\sin{\\frac{n \\pi x}{L}} &amp;= \\frac{B_n L}{2} \\nonumber\\\\ 2 \\frac{\\varphi_0}{L} \\int_0^L\\sin{{n \\pi x}{L}} &amp;= B_n \\nonumber\\\\ B_n &amp;= 2 \\frac{\\varphi_0}{L} \\big[\\frac{L}{n \\pi}(1-\\cos{n \\pi})\\big] \\nonumber\\\\ \\Aboxed{B_n &amp;= 2 \\frac{\\varphi_0}{n \\pi} (1-\\cos{n \\pi})} \\end{aligned}\\] For even values of \\(n\\), the \\(B_n\\) constant is zero. For odd values of \\(n\\), \\(B_n = \\frac{4 \\varphi_0}{n \\pi}\\). We utilize the values we acquired for \\(A_n\\), \\(B_n\\), and \\(\\lambda\\) and plug them into Eq. \\[eq:DiffSolFull\\]. A change in summation index to account for the \\(B_n\\) values yields: \\[\\begin{aligned} \\Aboxed{\\varphi(x,t) = \\frac{4 c_0}{\\pi} \\sum_{k=0}^\\infty \\frac{1}{2k+1} \\sin{\\frac{(2k+1)\\pi x}{L}}\\exp{\\Big[-\\big(\\frac{(2k+1)\\pi}{L}\\big)^2 Dt\\Big]}}\\end{aligned}\\] This summation converges quickly. We now have the ability to calculate the function \\(\\varphi(x,t)\\) at any position \\(0 &lt; x &lt; L\\) and time \\(t &gt; 0\\)! 3.6.2.3 Method of Integrating Factors is a technique that is commonly used in the solving of first-order linear ordinary differential equations (but is not restricted to equations of that type). In thermodynamics, it is used to convert a differential equation that is not exact (i.e., path-dependent, See Sec. 1.5.2) to an exact equation, such as in the derivation of entropy as an exact differential (Release TBD). 3.6.2.4 Fourier Integral Transforms This section will introduce an extremely powerful technique in solving differential equations: the Fourier transform. This technique is useful because it allows us to transform a complicated problem — a boundary value problem — into a simpler problem which can often be approached with ODE techniques or even algebraically. There are many excellet sources provided for this section, listed below. José Figueroa-O’Farrill’s wonderful Integral Transforms from Mathematical Techniques III at the University of Edinborough. W.E Olmstead and V.A. Volpert’s Differential Equations in Applied Mathematics at Northwestern University. J.J. Hoyt’s chapter on the Mathematics of Diffusion in his Phase Transformations text. Paul Shewman’s Diffusion in Solids. J.W. Brown and R.V. Churchill’s Fourier Series and Boundary Values Problems, 6th Edition. The primary goal behind the Fourier transform is to solve a differential equation with some unknown function \\(f\\). We apply the transform (\\(\\mathscr{F}\\)) to convert the function into something that can be solved more easily: \\(f \\xrightarrow{\\mathscr{F}} F\\). The transformed function is often also represented using a \\(\\hat{f}\\). We solve for \\(F\\) and then perform an inverse Fourier transform (\\(\\mathscr{F}^{-1}\\)) to recover the solution for \\(f\\). We find that Fourier series — which are used to when working with periodic functions — can be generalized to Fourier integral transforms (or Fourier transforms) when the period of the function becomes infinitely long. Let’s begin with the Fourier series an build on our results from our discussion above where we found that a continuous function \\(f(x)\\) defined on some finite interval \\(x \\in[0,L]\\) and vanishing at the boundaries, \\(f(0) = f(L) = 0\\) can be expanded as shown in \\[eq:DiffSolFull\\]. The following derivation is adapted from Olmstead and Volpert. In general, we can attempt to represent any function that is periodic over period \\([0,L]\\) with a Fourier series of form: \\[f(x) = a_0 + \\sum_{n=1}^\\infty\\left[a_n \\cos{\\frac{2 \\pi n x}{L}} + b_n \\sin{\\frac{2 \\pi n x}{L}}\\right] \\label{eq:GenSol}\\] However, we need to know how to find the coefficients \\(a_0\\), \\(a_n\\), and \\(b_n\\) for this representation of \\(f(x)\\). For this analysis we must utilize the following integral identities: \\[\\int_0^L{\\sin{\\frac{2 \\pi n x}{L}}\\cos{\\frac{2 \\pi n x}{L}}}dx= 0 \\quad n,m = 1,2,3,...,\\] \\[\\int_0^L{\\cos{\\frac{2 \\pi n x}{L}}\\cos{\\frac{2 \\pi m x}{L}}} dx= \\begin{cases} 0, \\text{\\,if} \\quad n,m = 1,2,3,..., n\\neq m\\\\ L/2, \\text{\\,if} \\quad n = m = 1,2,3,...,\\\\ \\end{cases}\\] \\[\\int_0^L{\\sin{\\frac{2 \\pi n x}{L}}\\sin{\\frac{2 \\pi m x}{L}}} dx = \\begin{cases} 0, \\text{\\,if} \\quad n,m = 1,2,3,..., n\\neq m\\\\ L/2, \\text{\\,if} \\quad n = m = 1,2,3,...,\\\\ \\end{cases}\\] \\[\\int_0^L{\\cos{\\frac{2 \\pi n x}{L}}} dx = \\begin{cases} 0, \\text{\\,if} \\quad n,m = 1,2,3,...,\\\\ L, \\text{\\,if} \\quad n = 0\\\\ \\end{cases}\\] \\[\\int_0^L{\\sin{\\frac{2 \\pi n x}{L}}} dx = 0, \\text{\\,if} \\quad n,m = 0,1,2,3,...,\\\\\\] These identities state the orthogonal properties of sines and cosines that will be used to derive the coefficients \\(a_0\\), \\(a_n\\), and \\(b_n\\). Recall that two functions are orthogonal on an interval if \\[\\int_a^b f(x)g(x)dx = 0\\] We can therefore multiply Eq. \\[eq:GenSol\\] by \\(\\cos{\\frac{2 \\pi x}{L}}\\) (note \\(n = 1\\)) and integrate over \\([0,L]\\): \\[\\begin{aligned} \\int_0^L f(x)\\cos{\\frac{2 \\pi x}{L}} dx &amp;= a_0 \\int_0^L \\cos{\\frac{2 \\pi x}{L}} dx +\\\\ &amp;a_1 \\int_0^L \\cos{\\frac{2 \\pi x}{L}} \\cos{\\frac{2 \\pi x}{L}} dx +b_1 \\int_0^L \\sin{\\frac{2 \\pi x}{L}} cos{\\frac{2 \\pi x}{L}} dx +\\\\ &amp;a_2 \\int_0^L \\cos{\\frac{4 \\pi x}{L}} \\cos{\\frac{2 \\pi x}{L}} dx +b_2 \\int_0^L \\sin{\\frac{4 \\pi x}{L}} cos{\\frac{2 \\pi x}{L}} dx + ...\\\\\\end{aligned}\\] Applying the orthogonal properties of the integral products finds that all terms on the right-hand side of this equation are zero apart from the \\(a_1\\) term. The equation therefore reduces to: \\[\\int_0^L f(x)\\cos{\\frac{2 \\pi x}{L}} dx = a_1 \\int_0^L \\cos{\\frac{2 \\pi x}{L}} cos{\\frac{2 \\pi x}{L}} dx= a_1\\frac{L}{2}\\] and \\[a_1 = \\frac{2}{L} \\int_0^L f(x)\\cos{\\frac{2 \\pi x}{L}} dx\\]. The other Fourier coefficients can be solved for in a similar manner, which yields the general solutions: \\[\\begin{aligned} a_0 &amp;= \\frac{1}{L} \\int_0^L f(x)\\\\ a_n &amp;= \\frac{2}{L} \\int_0^L f(x)\\cos{\\frac{2 n \\pi x}{L}}dx \\quad (n = 1,2,3...)\\\\ b_n &amp;= \\frac{2}{L} \\int_0^L f(x)\\sin{\\frac{2 n \\pi x}{L}}dx\\quad (n = 1,2,3...)\\end{aligned}\\] To this point we’ve solved, generally, for the coefficients of a Fourier series over a finite interval. This is useful, but we my want to use the full complex form of the Fourier series in later discussion of the Fourier transform. We know that Euler’s formula can be used to express trigonometric functions with the complex exponential function: \\[\\begin{aligned} \\sin{\\frac{2 \\pi n x}{L}} &amp;= \\frac{1}{2i}\\left(e^{i\\frac{n \\pi x}{L}}+e^{-i\\frac{n \\pi x}{L}}\\right) \\nonumber \\\\ \\cos{\\frac{2 \\pi n x}{L}} &amp;= \\frac{1}{2}\\left(e^{i\\frac{n \\pi x}{L}}+e^{-i\\frac{n \\pi x}{L}}\\right) \\label{eq:Euler}\\end{aligned}\\] and we define the wavenumbers to be: \\[k_n = 2 \\pi n/L \\quad n=0,1,2,..., \\label{eq:Wavenumber}\\] and therefore Eq. \\[eq:Euler\\] is written as: \\[\\begin{aligned} \\sin{k_n x} &amp;= \\frac{1}{2i}\\left(e^{i k_n x}+e^{-i k_n x}\\right) \\nonumber \\\\ \\cos{k_n x} &amp;= \\frac{1}{2}\\left(e^{i k_n x}+e^{-i\\ k_n x}\\right) \\label{eq:Euler}\\end{aligned}\\] This allows us to write the complete Fourier series (Substitute Eq. \\[eq:Euler\\] into \\[eq:GenSol\\]): \\[f_{L}(x) \\sim \\sum_{n=-\\infty}^{\\infty} c_n e^{i k_n x} \\label{eq:ComplexFourierSeries}\\] For convenience, we’ll define the integral to be \\([-\\frac{L}{2},\\frac{L}{2}]\\). The \\(\\sim\\) notation indicates that the series representation is an approximation, and the \\(L\\) represents the period over which the series is applied. The orthogonality condition holds for these complex exponential and its complex conjugate over this interval: \\[\\int_{-\\frac{L}{2}}^{\\frac{L}{2}} e^{i k_n x} e^{-i k_m x} dx = \\begin{cases} 0, \\text{\\,if} \\quad n \\neq m\\\\ L, \\text{\\,if} \\quad n = m\\\\ \\end{cases}\\] Therefore, if we multiply Eq. \\[eq:ComplexFourierSeries\\] by \\(e^{-i k_m x}\\) and solve we find the only term that survives is when \\(n=m\\): \\[\\int_{-\\frac{L}{2}}^{\\frac{L}{2}} f_L(x) e^{-i k_m x} dx = L c_m.\\] We can revert the place-keeping subscript \\(m\\) to \\(n\\) and solve for the Fourier coefficients to find: \\[c_n = \\frac{1}{L}\\int_{-\\frac{L}{2}}^{\\frac{L}{2}} f_L(x) e^{-i k_n x} dx \\quad \\text{for\\,} n = 0, \\pm1, \\pm2,... \\label{eq:FourierComps}\\] Alright, we’ve defined an interval \\([-L/2, L/2]\\), but we want to investigate this interval as \\(L \\rightarrow \\infty\\) in an attempt to eliminate the periodicity of the Fourier series. If \\(L \\rightarrow \\infty\\), we know that our function \\(f_L(x)\\) will be non-zero over only a very small range — say an interval of \\([-a/2, a/2]\\) where \\(a &lt;&lt; L\\). This means that \\[f_L{x} = \\begin{cases} 1 \\quad \\text{for} |x|&lt;a/2\\\\ 0 \\quad \\text{for} a/2 &lt; |x| &lt; L/2 \\end{cases} \\label{eq:Linfty}\\] This function is zero except for a small bump at the origin of height 1 and width \\(a\\). Let’s assess our function over the non-zero interval: \\[\\int_{-\\frac{a}{2}}^{\\frac{a}{2}} e^{-i k_n x} dx = \\frac{\\sin k_n a/2}{k_n L/2} = \\frac{\\sin n \\pi a/L}{n \\pi}. \\label{eq:Thing}\\] But how does this allow us to consider a continuous Fourier integral transform? Well, we need to consider the function \\(f_L(x)\\) as \\(L \\rightarrow \\infty\\). By doing so, we drive all the harmonics of the Fourier function — apart from the central one — out beyond infinity. \\(f(x)\\) then has a single bump of width \\(L\\) centered at the origin. That is, the separation between the \\(n\\) harmonics goes to zero, and the representation contains all harmonics. Further, as \\(L \\rightarrow \\infty\\) we no longer have a periodic function and we can i.e., the fundamental period becomes so large that we no longer have non-periodic function at all. This allows us to transition from a discrete description to a continuum and our Fourier sum can now be described as a Fourier integral. Recall the Fourier series (Eq/ \\[eq:ComplexFourierSeries\\]): \\[f_{L}(x) \\sim \\sum_{n=-\\infty}^{\\infty} c_n e^{i k_n x}\\] Which we now write as: \\[f_{L}(x) \\sim \\sum_{n=-\\infty}^{\\infty} \\frac{\\Delta k}{2\\pi/L} c_n e^{i k_n x} = \\sum_{n=-\\infty}^{\\infty} \\frac{\\Delta k}{2\\pi} L c_n e^{i k_n x} \\label{eq:Thing2}\\] where \\(\\Delta k = 2\\pi/L\\) is the difference between successive values of \\(k_n\\). We now define a function \\(F(k)\\) as \\[F(k) \\equiv \\Lim{L \\rightarrow \\infty} L c_n = \\Lim{L \\rightarrow \\infty} L c_{kL/2 \\pi} \\label{eq:DefFourierTransform}\\] Combining this definition with Eq. \\[eq:Thing\\] gives: \\[F(k) = \\frac{\\sin{(ka/2)}}{k/2}\\] and as \\(L \\rightarrow \\infty\\), Eq. \\[eq:Thing2\\] goes as \\[\\begin{aligned} f(x) &amp;= \\Lim{L \\rightarrow \\infty} \\sum_{n=-\\infty}^{\\infty} \\frac{\\Delta k}{2\\pi} L c_n e^{i k_n x} \\\\ \\Aboxed{f(x) &amp;= \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty}F(k) e^{ikx} dk} \\label{eq:FourierInversion}\\end{aligned}\\] The function \\(F(x)\\) is the Fourier transform of \\(f(x)\\) and Eq. \\[eq:FourierInversion\\] as a continuous superposition of Fourier component, with each component now represented by a continuous function \\(f(x)\\). Similarly, from Eq. \\[eq:DefFourierTransform\\] and Eq. \\[eq:FourierComps\\]: \\[\\begin{aligned} \\Aboxed{F(k) &amp;= \\int_{\\infty}^{\\infty} f(x) e^{-i k x} dx} \\label{eq:FourierTransform}\\end{aligned}\\] Eq. \\[eq:FourierTransform\\] is non-periodic analog to the expression for deriving the Fourier coefficients \\(c_n\\) in the periodic case. We call this function the Fourier (Integral) Transform of the function \\(f(x)\\) and it is often written as \\[F(k) \\equiv \\mathscr{F}\\big[f(t)\\big] \\label{eq:InversionFormula}\\] Similarly, Eq. \\[eq:InversionFormula\\] is known as the Inversion Formula or Inverse Fourier (Integral) Transform and is used to return the Fourier-transformed function from frequency space. It is often represented as: \\[f(x) \\equiv \\mathscr{F}^{-1}\\big[F(k)\\big]\\] Example: Let’s do a simple Fourier transform of a square-integrable function (this condition establishes that the function has a Fourier transform). We’ll try a square pulse over the interval \\[-, \\]: \\[f(x) = \\begin{cases} 1, \\text{\\,if\\,} |x| &lt; \\pi\\\\ 0, \\text{\\,otherwise}\\\\ \\end{cases}\\] We take the Fourier integral transform over the non-zero interval: \\[\\begin{aligned} F(x) &amp;= \\frac{1}{2 \\pi}\\int_{-\\infty}^{\\infty} f(x) e^{-i k x} dx\\\\ &amp;= \\frac{1}{2 \\pi}\\int_{-\\pi}^{\\pi} e^{-i k x} dx\\\\ &amp;= -\\frac{1}{2i \\pi k} e^{-i k x}\\Big|_{-\\pi}^{\\pi}\\\\ &amp;= -\\frac{1}{2i \\pi k} (e^{-i k \\pi}-e^{i k \\pi})\\\\ &amp;= \\frac{\\sin{\\pi k }}{\\pi k} \\end{aligned}\\] Integral transforms will prove massively useful in solving boundary value problems in the MAT_SCI core. Example One example is diffusion in the thin film problem. Imagine that there is thin region of finite width with high concentration of some species B situated between two “infinite” (thick) plate of pure A (after Hoyt, 1-6). Diffusion from the thin film is allowed to proceed over time into the adjacent plates. The thin film is centered at \\(x = 0\\), so the concentration profile will be an even function \\([\\varphi(x,t) = \\varphi(-x,t)]\\) How do we solve for the evolution of the concentration profile over time? This is an example in which we want to interpret this geometry as one with infinite period. When doing so we should consider using a Fourier integral transform. From the section above, we understand that the concentration profile \\(c(x,t)\\) can be obtained from the inverse transform of the Fourier space function \\(\\Phi(k,t)\\). Above, we derived the full Fourier integral transform, but here we know that the function is even, and so we can perform a Fourier Cosine integral transform, which simplifies the mathematics and allows us to perform the transform from \\([0,\\infty]\\). The following derivation is after Hoyt, Ch. 1-8. \\[\\begin{aligned} f(x) = \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} F(x) \\cos{(kx)} dk\\\\ F(x) = \\frac{1}{\\pi} \\int_0^{\\infty} f(x) \\cos{(kx)} dx\\end{aligned}\\] In our case, we have: \\[\\begin{aligned} \\varphi(x,t) = \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} \\Phi(k,t) \\cos{(kx)} dk\\\\ \\Phi(k,t) = \\frac{1}{\\pi} \\int_0^{\\infty} \\varphi(x,t) \\cos{(kx)} dx \\label{eq:OddSol1}\\end{aligned}\\] The utility of utilizing the Fourier intergral transform is that the PDEs in space and time can be converted to ODEs in the time domain alone, which are often much easier to solve. The ability for us to do this hinges on a key property of a Fourier transform that relates the Fourier transform of the \\(n\\)th derivative of a function to the Fourier transform of the function itself. \\[\\mathscr{F}\\big[f^{(n)}(x)\\big](k) = (ik)^{n}\\mathscr{F}\\big[f(x)\\big](k) \\label{eq:}\\] This, as you will see, allows us to convert a PDE in \\(t\\) and \\(x\\) to a ODE in \\(t\\) alone. Let’s apply this property to the 1D diffusion equation. First, we know we are performing a Fourier transform in \\(x\\), so the time derivative can be pulled from the integral on the left-hand side of the equation. \\[\\begin{aligned} \\mathscr{F}[\\Partial{}{\\varphi(x,t)}{t}] &amp;= \\frac{1}{\\pi}\\int_{0}^{\\infty} \\Partial{}{\\varphi(x,t)}{t} \\cos{(-ikx)} dx\\\\ &amp;= \\frac{1}{\\pi}\\Partial{}{}{t}\\left[\\int_{0}^{\\infty}\\varphi(x,t)\\cos{(-ikx)} dx\\right]\\\\ &amp;= \\frac{1}{\\pi}\\Partial{}{}{t}\\left[\\Phi(k,t)\\right]\\end{aligned}\\] and the right-hand side of the equation is: \\[\\begin{aligned} \\mathscr{F}\\big[D\\Partial{2}{}{x}\\varphi(x,t)\\big] &amp;= (ik)^2D\\mathscr{F}\\big[\\varphi(x,t)\\big]\\\\ &amp;= -D\\frac{k^2}{\\pi}\\int_0^{\\infty}\\varphi(x,t)\\cos{(-ikx)} dx\\\\ &amp;= -D\\frac{k^2}{\\pi} \\Phi(k,t)\\end{aligned}\\] and therefore: \\[\\begin{aligned} \\frac{1}{\\cancel{\\pi}}\\Partial{}{}{t}\\left[\\Phi(k,t)\\right] &amp;= -D\\frac{k^2}{\\cancel{\\pi}} \\Phi(k,t)\\\\ \\Aboxed{\\Partial{}{}{t}\\left[\\Phi(k,t)\\right] &amp;= -Dk^2 \\Phi(k,t)}\\end{aligned}\\] This differential equation can be solved by inspection3 to be: \\[\\Phi(k,t) = A^{0}(k) e^{-k^2Dt} \\label{eq:Sol1}\\] Where \\(A^0(k)\\) is a constant that that defines the Fourier space function \\(\\Phi\\) at \\(t=0\\). To fully solve this problem and derive \\(\\varphi(x,t)\\) we must next solve this value \\(A^0(k)\\) and apply the inverse Fourier transform. Let us consider our initial condition. Our concentration profile can be modeled as a \\(delta\\)-function concentration profile, \\(\\varphi(x,0) = \\alpha \\delta(x)\\)) fixed between two infinite plates, where the integrated concentration is \\(\\alpha\\): \\[\\int_{\\infty}^{\\infty} \\varphi(x,0)dx = \\int_{\\infty}^{\\infty} \\alpha \\delta(x) dx = \\alpha\\] The constant \\(A^0(k)\\) is, at \\(t = 0\\), defined by Eq. \\[eq:Sol1\\] and Eq. \\[eq:OddSol1\\] to be: \\[\\begin{aligned} \\Phi(k,t) &amp;= A^{0}(k)e^{0}\\\\ &amp;= \\frac{1}{2\\pi}\\int_{0}^{\\infty}\\varphi(x,t) \\cos{(kx)} dx\\end{aligned}\\] Inserting the delta function for (x,t) yields: \\[A^{0}(k) = \\frac{\\alpha}{\\pi}\\int_{0}^{\\infty} \\delta(x) \\cos{(kx)} dx\\] We’ll take advantage of the evenness of this function and instead integrate over \\([\\infty, \\infty]\\). This allows us to avoid the messiness at \\(x=0\\) as well circumvent using the Heaviside step function. \\[\\begin{aligned} A^{0}(k) &amp;= \\frac{\\alpha}{2\\pi}\\int_{-\\infty}^{\\infty} \\delta(x) \\cos{(kx)} dx\\\\ A^{0}(k) &amp;= \\frac{\\alpha}{2\\pi} \\cos{(0)} dx\\\\ %Deltafunction fundamental property \\Aboxed{A^{0}(k) &amp;= \\frac{\\alpha}{2\\pi}}\\end{aligned}\\] Finally, now that we have \\(A^{0}\\), we must perform the inverse transformation to find the expression for \\(\\varphi(x,t)\\). \\[\\begin{aligned} \\varphi(x,t) &amp;= \\frac{\\alpha}{2\\pi}\\int_{-\\infty}^{\\infty} e^{-k^2Dt} \\cos{(kx)} dk\\\\ \\intertext{This can be completed through integration by parts, trigonometric identities, and completing the square... for explicit step-by-step analysis use Wolfram$|$Alpha or \\href{http://www.integral-calculator.com/}{Scherfgen&#39;s Integral Calculator}. Let&#39;s state the result:} \\varphi(x,t) &amp;= \\frac{\\alpha}{2\\sqrt{\\pi D t}} e^{-x^2/4Dt}\\end{aligned}\\] This is a Gaussian distribution centered at \\(x = 0\\) and which increases in width with time. This is good — it certainly makes sense intuitively. 3.6.2.5 Bessel Functions 3.6.2.6 Legendre Polynomials 3.6.2.7 Euler’s Method 3.6.3 Solving Second-order Linear ODEs (Release TBD) Principle of Superposition Series Solutions 3.6.4 Laplace Transforms (Release TBD) 3.6.5 Stability Theory (Release TBD) This is somewhat dated. I’m looking for a more up-to-date version.↩︎ You can also create a MATALB function that accepts inputs and can be called from the command line.↩︎ If you don’t see this, that’s fine, review Separation of Variables — this equation is separable↩︎ "],["coding.html", "4 Coding 4.1 MATLAB (Release 2/2016)", " 4 Coding 4.1 MATLAB (Release 2/2016) Encountered in: MAT_SCI 405, 408 MATLAB is a powerful computing environment that is commonly used by scientists and engineers of all disciplines for numerical simulation, data analysis, data visualization, hardware control, and creation of custom graphical user interfaces. MATLAB’s built-in functional utility is vast, and this is further supported by an active user community that interacts at the MathWorks File Exchange. There, you can find free user-generated content — functions, applictions, examples, drivers, etc, that may prove useful in your coursework and research. MATLAB is installed on the computers in the Bodeen Laboratory and is also available for purchase by students ($72/yr on campus, including VPN; $130 off campus). Many research groups also have license subscriptions that can be utilized for your academic projects. Below is a basic introduction to working with MATLAB. Some good resources and tutorials are linked below for further investigation. Suggested Resources: Introduction to MATLAB, David Houcque, Northwestern University (2005) eReserve Link4 Ilya Mikhelson (Northwestern EECS) has developed a series of MATLAB tutorials and posted them on YouTube. Specifically useful tutorials are following MATLAB Script Tutorial MATLAB Indexing Tutorial MATLAB Logical Expression Tutorial MATLAB Subarray Tutorial MATLAB Concatenation Tutorial MATLAB Variable Declaration Tutorial MATLAB For Loops Tutorial and MATLAB While Loops Tutorial MATLAB Function Tutorial MATLAB Debugging Tutorial 4.1.1 Interfacing with MATLAB When a user first opens MATLAB (this guide is written for the current version of MATLAB as of 2/2017: MATLAB 9.1) as shown in Fig. 1 with various sub-windows visible. The MATLAB user interface, MATLAB 9.1. This window (at the far left) shows the files that are present in your current directory. This can be changed by navigating to a different directory, shown in the field directly above the windows. This window allows you to directly input commands in a command-line format. This is useful for simple, quick calculations and plots. For more extensive calculations you will need to write a script in the Editor. You will use this space to write and edit your code. While you can, in principle, edit MATLAB code in any text editor, MATLAB’s editor is ideally suited to editing its own code and provides utilities that facilitate your scripting, including color-coded syntax, debugging options, and script comparison tools. These scripts are saved as so-called “m-files”, with extension .m. The workspace contains data structures (scalars, vectors, matrices, tensors, and structure/cell arrays) that are created during your session. The woskpace provides you with at-a-glance information about, for example the data name, value, class, size, and statistics. You can view the data values by double-clicking on the data icon. This will open a sub-tab within the editor window which allows you to easily access and edit your data. 4.1.2 Basic Commands and Arithmetic Syntax MATLAB possess its own scripting language and possess unique syntax. Below we list a selection — and some examples — of the most basic MATLAB commands. When appropriate, you can copy these examples directly into the command line or m-file and execute. The output will appear in the Command Window. This list is sourced from a document compiled from the \\(\\Eta \\Kappa \\Nu\\) society at the University of Minnesota. The full source can be found here. When operating in the command window there are a useful set of session managing commands that will streamline your workflow. These are listed in Tab. 1. Syntax for managing MATLAB sessions. Description Syntax Example Session Management Operations Clear command window clc Clear variables clear Returns help page help help circshift Set global variable global global x Quit MATLAB quit Display/use most resent answer ans ans+2 Simple arithmetic operations and special variables are tabulated in Fig. 2. Arithmetic MATLAB syntax and special variables. Description Syntax Example Arithmetic Operations and Special Variables Addition + 1+1 Subtraction - 1-1 Multiplication * 2*1 Right-Hand Division / 1/2 Left-Hand Division ! 2! Exponentiation ^ e^2 Assignment = a = 1 Grouping ( ) 2*(3+6) The number \\(\\pi\\) pi 2*pi Imaginary number \\(\\sqrt{-1}\\) i,j 1 + 3i Infinity Inf 1/0 Not-a-Number NaN Inf/Inf 4.1.3 Matrix Notation and Operations MATLAB is recognized for its strength in writing, representing, and operating on arrays. The construction of matrices in MATLAB is straightforward — examples are shown in Tab. 3. When entering the example commands in the command line, a matrix-form output will be created. You can also see and edit these matrices by using the Workspace and accessing the Variable Spreadsheet. MATLAB syntax for creating and operating on matrices. To test operations possesing A, B, X or Y you must first create those matrices. Description Syntax Example Array Creation Row Vector [. . .] a = [1 2 3 4] Column Vector [. ; . ; .] a = [1; 2; 3; 4] 2 \\(\\times\\) 2 Matrix [. . ; . . ;] a = [1 2; 3 4] Create \\(n \\times m\\) matrix ones(n,m) randi([-b,b],n,m) ones(3,5) rand([-5,5],4,4) Accessing Matrix Elements Access Row \\(i\\), column \\(j\\) of \\(A\\) A(i,j) A(3,2) Access \\(i\\)th row of \\(A\\) A(i,:) A(1,:) Access \\(j\\)th column of \\(A\\) A(:,j) A(:,2) Access submatrix of \\(A\\) A([i1 i2],[j1, j2 ,3]) A([1 2], [1 2 4]) Matrix Operations Matrix Transpose A' Matrix Inverse inv(A) Matrix Multiplication A*B Matrix Inverse Multiplication A/B or A*inv(B) Matrix Exponentiation to \\(n\\) A^n Matrix Cross-product cross(A,B) Matrix Dot-product dot(A,B) Element-by-element operations A./B A.^n Return \\(n \\times m\\) size of A size(A) Concatenate column X onto A [A X] Concatenate row Y onto A [A; Y] 4.1.4 Creating and Using Functions MATLAB has a large number of built-in function that can provided with input and provide an output. Simple examples include cos(x) or sqrt(x). When functions are called from script files they must be saved as individual files within the working directory (or a directory which has been added to the path). Example syntax for defining a “Power” function, which takes a input variable and outputs the computed result is shown below. This function can be called within a script to return variable p. The syntax here is: function [out1,out2, ..., outN] = myfun(in1,in2,in3, ..., inN), where in1 ... inN are input variables and out1, ..., outN are output variables. Autonomous functions are another way to define a user-supplied function without necessitating the creation of a new file when calling them in a script. These can be defined in the command line or in-line within the m-file. An example of use of an autonomous function is shown in Fig. 0.1.7. %% Power Function % This function calculates x to the nth power. % Returns variable p. function [p] = Power(x,n) p = x.^n; end 4.1.5 Control Flow and Iterative Loops A core component of any numerical calculation is the iterative loop that controls the simulation steps. These statements direct MATLAB to perform a certain calculation either while certain conditions are true or for certain a specific range/specific number of times. The general syntax here is disp for index = values statements end Below, we show an example of a for loop that simply displays the values over each iteration — here from -2 (start) to 4 (finish) in steps of 0.5 (del). In place of the disp command you can place any command or series of commands, of course. %% Define variables and preallocate. start = -2; %Beginning of range . finish = 4; %End of range del = 0.5; %Interval A = zeros (( finish-start )/del ,1); %Preallocated column vector , defined by range and interval . index = 1; %Starting index forvector A. %% For loop and define vector elements for n = start : del : finish %Range from start to finish with interval del index = index + 1; %Step index A(index ,1) = n; %Define element end disp (A); A while loop is shown below. This statement is useful when you are unsure about when the condition may become untrue (e.g. convergence criteria, etc.). The same example as shown above is show below, but now for the case which the index is less than or equal to the number of elements in the matrix. This job is more suitable for a for loop, but we show it below using a while loop for comparison. %% Define variables and preallocate. n = -2; %Beginning of range. finish = 4; %End of range del = 0.5; %Interval A = zeros((finish-n)/del,1); %Preallocated column vector, defined by range and interval. index = 1; %Starting index for vector A. %% For loop and define vector elements while index &lt;= length(A(:,1)) %Condition for calculation n = n + del; A(index,1) = n; %Define element index = index + 1; %Step index end disp(A); %Display A in command window. 4.1.6 Basic Plotting You will inevitably want to visualize your data or computational results. MATLAB has a variety of tools to facility this. There are many, many options to plot data. Below we show a simple 2D plot that has some plotting options defined including axis ranges and labels. The plot command mesh renders a wireframe figure of a scaled \\(sinc\\) function with \\(Z\\) values plotted with color. Thre sult is shown in Fig. 2 \\(X\\) and \\(Y\\) values are vectors that describe the plane. The example below shows a full, annotated m-file in which the last section (the plot section) defines plotting options including axis for axis range and xlabel for axis labels. %% MeshPlot.m % A simple script with an anonymous function to plot a radial sinc function. % Author: JD Emery % Modified: Feb 18th 2017 %% Variable Declaration/Preallocation a = 2; b = 3; %% Anonymous Functions sinc2 = @(x,y) x*sin(y)./y; %Define anonymous function sinc2 with inputs (x,y) %% Script [X,Y] = meshgrid(-20:.25:20); %Define the mesh calculation range. R = sqrt(a*X.^2 + b*Y.^2); %Define value R in terms of X,Y meshpoints. %% Plot f = figure; %Create figure and define handle. set(f,&#39;name&#39;,&#39;Sinc Function 2D&#39;,&#39;numbertitle&#39;,&#39;off&#39;) %Name figure mesh(X,Y,sinc2(a,R),&#39;LineWidth&#39;,0.3) %Plot [X,Y,Z] using mesh tool. axis([-8 8 -8 8 -1 2]) %Set plotranges xlabel(&#39;-8 &lt; x &lt; 8&#39;) % x-axis label ylabel(&#39;-8 &lt; y &lt; 8&#39;) % y-axis label zlabel(&#39;sinc(x*R)&#39;) %z-axis label A meshplot of a sinc function. 4.1.7 Scripting and Debugging the m-file Working with the command line is convenient for quickly accessing and working with data, or running scripts or creating custom functions. The examples above can be simply executed line-by-line in the the command window (although they were written as m-files), in doing so you will soon find, however, that it is much more convenient to write a script — a series of commands — in a m-file within the MATLAB editor. A script m-file operates simply by executing any commands listed in the file. A somewhat more advanced example is shown in Section 0.1.6. A very simple example for an m-file that uses the disp command to print “Hello World”in the command window is shown below. The example below also contains a commented-out % preamble with file information as well as section breaks %% for organization. %% HelloWorld.m % A simple script for &quot;Hello World&quot; % Author: JD Emery % Modified: Feb 18th 2017 %% disp(&#39;Hello World&#39;) Script m-files usually possess at least four sections (examples of all these sections, as well as a plotting section are shown in 0.1.6): This is where you will record pertinent notes about the file such as version history, authorship, licensing, or other comments. These are usually commented out with % symbols. After the preamble there is typically a section where you define the variables that will be used in the script5. Preallocation of arrays with zeros(n,m) or ones(n,m) can greatly reduce the amount of memory consumed by MATLAB — especially when arrays iteratively grow when the script is run. Often, you will want to add a function that is not built into MATLAB syntax or that does not need its own program file. These functions are often added at the end the m-file and are accessed when the script is run. This is where the calculations are performed and the data is saved or plotted. MATLAB provides a number of interactive tools to simplify coding and debugging. Useful tools exist in the editor ribbon, including insert of sections of functions (from a library) or indent and comment control. Arguably, the most valuable tool is the setting and handling of breakpoints. Breakpoints are especially useful when constructing loops because the calculations can be monitored step-by-step through the loop. Debugging MATLAB scripts. To set a breakpoint simply left click on the horizontal dash next to the line you would like to break at (indicated with a red arrow, Fig. 3). Run the code. Your script will stop at this point and you can investigate variable values. Then, you can advance the code to the next breakpoint (Fig. 3, green square). When in a loop this command will iterate the loop one step. When ready, you can use the breakpoints button to eliminate all breakpoints and run the code. This is somewhat dated. I’m looking for a more up-to-date version.↩︎ You can also create a MATALB function that accepts inputs and can be called from the command line.↩︎ "],["release-history.html", "A Release History", " A Release History Version 2016.1, 2016-11: Initial release. Version 2017.1, 2017-01: Added tensors, summation notation, and coordinate transformation. Version 2017.2, 2017-02: Added Fourier integral transforms. Version 2017.3, 2017-02: Added MATLAB introduction and resources. Version 2020.1, 2020-07: Uploaded to Overleaf to solicit student/faculty contribution. "]]
